{"config":{"lang":["en"],"min_search_length":3,"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"Welcome to the Minecraft UROP team # This documentation is written to help you set up and navigate our project repository. You are recommended to follow the documentation pages in sequence to learn the background, the human behaviors, the codes and the tools. This documentation is being developed as your UROP progresses, so don't hesitate to ask Yang via email or Slack if you have any questions. Upcoming Zoom talks # During each week's Mon 2-3 pm meetings, everyone will share their progress, blocks, and ideas in the first half an hour. Then we will hear a half-hour talk from a team member to learn from each other's approach. There might also be special office-hour (OH) Talks given to guide you specific technical aspects. Week 6 (July 6th) - UROP students each has 15 minutes to present on player strategy/type observed in ASU test subjects . 4-15 pages of slides are recommended. Plan for computational model is optional. Presenter order: Anna, Isabelle, Christian, and Darren. Week 7 (July 13th) - Week 8 (July 20th) - Week 9 (July 27th) - Week 10 (August 3rd) - Week 11 (August 10th) - Week 12 (August 17th) - Week 13 (August 24th) - Week 14 (August 31st) - Past Zoom talks: Week 1 (June 2nd) - How do humans and our agents infer player strategies, beliefs, and future actions (Yang) Week 2 (June 8th) - Test case demonstrations (Yang) Week 2 (June 9th) - (Tue OH 1:30 pm) Q&A on prerequisite reading materials ; Tutorial on numpy and pytorch (Tejas) Week 2 (June 12th) - (Fri OH 2 pm) Tutorial on the implementation of value iteration and policy iteration algorithms using numpy (Tejas) Week 3 (June 15th) - MCTS & A* method for the gridworld search task (Essie) Week 5 (June 29th) - Temporal planning (Yuening) Meetings and office hours (EST) # All meetings happen at Yang's Zoom room, https://mit.zoom.us/j/8514113919 . The slots are scheduled based on our when2meet survey result. Please keep it updated. Weekly team meeting time: Mondays 2-3 pm (Weekly update meeting) Fridays 6-9 pm (Team Minecraft SAR mission) Office hours: Tuesdays 1:30-2:30 pm (with Tejas and Yang) Fridays 2-3 pm (with Tejas and Yang) Individual meeting times (Starting on Week 2): Thursdays 2 pm: Darren (with Yuening) Thursdays 3:30 - 4:30 pm: Anna and Isabelle (with Essie) Thursdays 5 pm: Christian (with Tejas)","title":"Welcome"},{"location":"#welcome-to-the-minecraft-urop-team","text":"This documentation is written to help you set up and navigate our project repository. You are recommended to follow the documentation pages in sequence to learn the background, the human behaviors, the codes and the tools. This documentation is being developed as your UROP progresses, so don't hesitate to ask Yang via email or Slack if you have any questions.","title":"Welcome to the Minecraft UROP team"},{"location":"#upcoming-zoom-talks","text":"During each week's Mon 2-3 pm meetings, everyone will share their progress, blocks, and ideas in the first half an hour. Then we will hear a half-hour talk from a team member to learn from each other's approach. There might also be special office-hour (OH) Talks given to guide you specific technical aspects. Week 6 (July 6th) - UROP students each has 15 minutes to present on player strategy/type observed in ASU test subjects . 4-15 pages of slides are recommended. Plan for computational model is optional. Presenter order: Anna, Isabelle, Christian, and Darren. Week 7 (July 13th) - Week 8 (July 20th) - Week 9 (July 27th) - Week 10 (August 3rd) - Week 11 (August 10th) - Week 12 (August 17th) - Week 13 (August 24th) - Week 14 (August 31st) - Past Zoom talks: Week 1 (June 2nd) - How do humans and our agents infer player strategies, beliefs, and future actions (Yang) Week 2 (June 8th) - Test case demonstrations (Yang) Week 2 (June 9th) - (Tue OH 1:30 pm) Q&A on prerequisite reading materials ; Tutorial on numpy and pytorch (Tejas) Week 2 (June 12th) - (Fri OH 2 pm) Tutorial on the implementation of value iteration and policy iteration algorithms using numpy (Tejas) Week 3 (June 15th) - MCTS & A* method for the gridworld search task (Essie) Week 5 (June 29th) - Temporal planning (Yuening)","title":"Upcoming Zoom talks"},{"location":"#meetings-and-office-hours-est","text":"All meetings happen at Yang's Zoom room, https://mit.zoom.us/j/8514113919 . The slots are scheduled based on our when2meet survey result. Please keep it updated. Weekly team meeting time: Mondays 2-3 pm (Weekly update meeting) Fridays 6-9 pm (Team Minecraft SAR mission) Office hours: Tuesdays 1:30-2:30 pm (with Tejas and Yang) Fridays 2-3 pm (with Tejas and Yang) Individual meeting times (Starting on Week 2): Thursdays 2 pm: Darren (with Yuening) Thursdays 3:30 - 4:30 pm: Anna and Isabelle (with Essie) Thursdays 5 pm: Christian (with Tejas)","title":"Meetings and office hours (EST)"},{"location":"data-trajectory/","text":"Dataset I - Human trajectory # The human trajectories and screen recordings are collected during 21 to 24 April. It contains 65 trajectories from 14 players and 51 video recordings . Each player was given five mazes with varying time limit based on their Minecraft expertise (to ensure that they feel time pressure and make strategic decisions). The time limit ranges from 105-240 seconds and is on average 150 seconds. The players found 2-8 of the 8 victims in each maze and found on average 5.6 victims. The experiment design and analysis can be found here . The instructions presented to the players can be found here Data format # Download the zipped trajectory data files from Dropbox link and unzip the folder in gridworld/recordingsMalmo/ , you will see a folder structure like this: gridworld/ recordingsMalmo/ Recoring replay.ipynb ## codes for selecting and cleaning data originals/ ## the original data collected in April player1 ***/ test3/ observations.txt ## the sequence of data, 20 entries per second missionInit.xml ## the mission initialization file test4/ ... player2 ***/ ... player21/ ## (ignore this) new data collected using the Stata Maze Inside the test folders, the original data files observations.txt collected from Malmo contain sequences of stats like the following: 20200417T204005.847011 { \"Name\":\"Cristina\",\"Score\":0,\"XP\":0,\"Food\":20, \"Life\":20.0,\"Air\":300,\"DamageTaken\":0,\"DamageDealt\":0, \"IsAlive\":true,\"TimeAlive\":26,\"WorldTime\":1,\"TotalTime\":31, \"DistanceTravelled\":0,\"MobsKilled\":0,\"PlayersKilled\":0, \"XPos\":0.5,\"YPos\":227.4215999984741,\"ZPos\":10.5,\"cell\":\"(1,11)\", \"Pitch\":0.0,\"Yaw\":0.0, \"LineOfSight\":{ \"hitType\":\"block\",\"type\":\"iron_block\", \"x\":0.49999999999999983,\"y\":229.12000000476837,\"z\":12.0, \"inRange\":true,\"distance\":1.5 }, \"Hotbar_0_size\":0,\"Hotbar_0_item\":\"air\",\"Hotbar_1_size\":0,\"Hotbar_1_item\":\"air\",\"Hotbar_2_size\":0,\"Hotbar_2_item\":\"air\",\"Hotbar_3_size\":0,\"Hotbar_3_item\":\"air\",\"Hotbar_4_size\":0,\"Hotbar_4_item\":\"air\",\"Hotbar_5_size\":0,\"Hotbar_5_item\":\"air\",\"Hotbar_6_size\":0,\"Hotbar_6_item\":\"air\",\"Hotbar_7_size\":0,\"Hotbar_7_item\":\"air\",\"Hotbar_8_size\":0,\"Hotbar_8_item\":\"air\", \"nearby\":[\"iron_block\",\"iron_block\",\"iron_block\",\"iron_block\",\"iron_block\",\"iron_block\",\"iron_block\",\"iron_block\",\"iron_block\"] } Using gridworld/recordingsMalmo/Recoring replay.ipynb to select data terms and entries, we generate json files as input to our framework and store them in gridworld/trajetctories/24by24/ . An example _player1 ciaran_test3.json looks like the following: { \"map\": \"test3.csv\", \"time_start\": \"T20224\", \"time_finish\": \"T20253\", \"duration\": 179, \"steps\": { \"0\": { \"time\": \"T20224\", \"x\": 0.9363693372703299, \"y\": 227.0, \"z\": 11.446659656203945, \"yaw\": -24.749998, \"cell\": \"(1,12)\", \"lineOfSight\": { \"hitType\": \"block\", \"type\": \"iron_block\", \"x\": 2.1135941049334743,\"y\": 227.85087919425717,\"z\": 14.0, \"inRange\": true, \"distance\": 2.9149532318115234 } }, \"1\": { ... }, ... } } Experiment replay # Using gridworld/visualize.py , we visualize the trajectories in color and generate the png files into gridworld/recordings/200429 Replay trace/ Experiment logistics # To conduct remote game sessions, we sent the following instructions to the participants after arranging the time with them: Great! The only thing that we need to do beforehand is To enable remote game control, please register for an Parsec account https://parsecgaming.com/signup/ Download the Parsec desktop app to enjoy smoother connection https://parsecgaming.com/downloads/ After you have created a parsec account and confirmed your email address, please send me your username shown on the top right of your account \"app\" page with a number string, which looks like ztyang#826739 Whenever we do the game, we can use this Zoom meeting room for communication https://mit.zoom.us/j/8514113919 Look forward to seeing you then!","title":"Dataset I - human trajectories"},{"location":"data-trajectory/#dataset-i-human-trajectory","text":"The human trajectories and screen recordings are collected during 21 to 24 April. It contains 65 trajectories from 14 players and 51 video recordings . Each player was given five mazes with varying time limit based on their Minecraft expertise (to ensure that they feel time pressure and make strategic decisions). The time limit ranges from 105-240 seconds and is on average 150 seconds. The players found 2-8 of the 8 victims in each maze and found on average 5.6 victims. The experiment design and analysis can be found here . The instructions presented to the players can be found here","title":"Dataset I - Human trajectory"},{"location":"data-trajectory/#data-format","text":"Download the zipped trajectory data files from Dropbox link and unzip the folder in gridworld/recordingsMalmo/ , you will see a folder structure like this: gridworld/ recordingsMalmo/ Recoring replay.ipynb ## codes for selecting and cleaning data originals/ ## the original data collected in April player1 ***/ test3/ observations.txt ## the sequence of data, 20 entries per second missionInit.xml ## the mission initialization file test4/ ... player2 ***/ ... player21/ ## (ignore this) new data collected using the Stata Maze Inside the test folders, the original data files observations.txt collected from Malmo contain sequences of stats like the following: 20200417T204005.847011 { \"Name\":\"Cristina\",\"Score\":0,\"XP\":0,\"Food\":20, \"Life\":20.0,\"Air\":300,\"DamageTaken\":0,\"DamageDealt\":0, \"IsAlive\":true,\"TimeAlive\":26,\"WorldTime\":1,\"TotalTime\":31, \"DistanceTravelled\":0,\"MobsKilled\":0,\"PlayersKilled\":0, \"XPos\":0.5,\"YPos\":227.4215999984741,\"ZPos\":10.5,\"cell\":\"(1,11)\", \"Pitch\":0.0,\"Yaw\":0.0, \"LineOfSight\":{ \"hitType\":\"block\",\"type\":\"iron_block\", \"x\":0.49999999999999983,\"y\":229.12000000476837,\"z\":12.0, \"inRange\":true,\"distance\":1.5 }, \"Hotbar_0_size\":0,\"Hotbar_0_item\":\"air\",\"Hotbar_1_size\":0,\"Hotbar_1_item\":\"air\",\"Hotbar_2_size\":0,\"Hotbar_2_item\":\"air\",\"Hotbar_3_size\":0,\"Hotbar_3_item\":\"air\",\"Hotbar_4_size\":0,\"Hotbar_4_item\":\"air\",\"Hotbar_5_size\":0,\"Hotbar_5_item\":\"air\",\"Hotbar_6_size\":0,\"Hotbar_6_item\":\"air\",\"Hotbar_7_size\":0,\"Hotbar_7_item\":\"air\",\"Hotbar_8_size\":0,\"Hotbar_8_item\":\"air\", \"nearby\":[\"iron_block\",\"iron_block\",\"iron_block\",\"iron_block\",\"iron_block\",\"iron_block\",\"iron_block\",\"iron_block\",\"iron_block\"] } Using gridworld/recordingsMalmo/Recoring replay.ipynb to select data terms and entries, we generate json files as input to our framework and store them in gridworld/trajetctories/24by24/ . An example _player1 ciaran_test3.json looks like the following: { \"map\": \"test3.csv\", \"time_start\": \"T20224\", \"time_finish\": \"T20253\", \"duration\": 179, \"steps\": { \"0\": { \"time\": \"T20224\", \"x\": 0.9363693372703299, \"y\": 227.0, \"z\": 11.446659656203945, \"yaw\": -24.749998, \"cell\": \"(1,12)\", \"lineOfSight\": { \"hitType\": \"block\", \"type\": \"iron_block\", \"x\": 2.1135941049334743,\"y\": 227.85087919425717,\"z\": 14.0, \"inRange\": true, \"distance\": 2.9149532318115234 } }, \"1\": { ... }, ... } }","title":"Data format"},{"location":"data-trajectory/#experiment-replay","text":"Using gridworld/visualize.py , we visualize the trajectories in color and generate the png files into gridworld/recordings/200429 Replay trace/","title":"Experiment replay"},{"location":"data-trajectory/#experiment-logistics","text":"To conduct remote game sessions, we sent the following instructions to the participants after arranging the time with them: Great! The only thing that we need to do beforehand is To enable remote game control, please register for an Parsec account https://parsecgaming.com/signup/ Download the Parsec desktop app to enjoy smoother connection https://parsecgaming.com/downloads/ After you have created a parsec account and confirmed your email address, please send me your username shown on the top right of your account \"app\" page with a number string, which looks like ztyang#826739 Whenever we do the game, we can use this Zoom meeting room for communication https://mit.zoom.us/j/8514113919 Look forward to seeing you then!","title":"Experiment logistics"},{"location":"howto-git/","text":"Git - Common Nightmares # It can be stressful to tackle problems with git because you risk destroying the team's codes. It's always good to search on Google/ Slack Overflow to find solutions and understand what the commands in the solutions mean. Here let's include the issues we've encountered and our solutions to fix them. Tried to push a large file and cannot push again # Can't remove file from git commit I solved by git filter-branch -f --index-filter \"git rm -rf --cached --ignore-unmatch FOLDERNAME\" -- --all git add * git commit -m 'fix large files' git pull origin master --allow-unrelated-histories git push","title":"Git - Common nightmares"},{"location":"howto-git/#git-common-nightmares","text":"It can be stressful to tackle problems with git because you risk destroying the team's codes. It's always good to search on Google/ Slack Overflow to find solutions and understand what the commands in the solutions mean. Here let's include the issues we've encountered and our solutions to fix them.","title":"Git - Common Nightmares"},{"location":"howto-git/#tried-to-push-a-large-file-and-cannot-push-again","text":"Can't remove file from git commit I solved by git filter-branch -f --index-filter \"git rm -rf --cached --ignore-unmatch FOLDERNAME\" -- --all git add * git commit -m 'fix large files' git pull origin master --allow-unrelated-histories git push","title":"Tried to push a large file and cannot push again"},{"location":"howto-review-literature/","text":"How to review the literature # Here is Dylan's notes from the discussions in the Genesis story understanding group. Triage with abstracts: If the paper seems like a merely incremental change on a topic you already know, you can skip it. You can practice making these snap assessments. Read authors, not papers: Focus on understanding the work of particular researchers in general so you can contextualize all of their papers without reading all of their papers. Breadth-first citation: Starting with a few stand-out papers, recursively follow the interesting references in breadth-first order. Ask an expert: Don't try to read everything. Ask someone who's read everything what you should read. Rely on networks of trusted people to highlight good papers. Read using effective-writing rules: For example, you can use VSNC to parse an abstract or paper. Alternate absorbing and synthesizing: Take time in between bouts of reading to restate what you've read, draw connections, imagine applications, and dream up new possibilities. Reading for understanding is a constructive process. 7a. Note what others think is relevant: When others suggest related work, they're partly telling you what they think your work is. If they're off-base, it's an opportunity to clarify your own area. (Pay attention to misunderstandings). 7b. Get past \"Isn't this just X?\": One particularly reductive form of misunderstanding is \"Isn't your work just [topic]?\". It isn't really actionable, as it's the consequence of a listener who has simply stopped too early. Tells you more about the listener than about how to present your work.","title":"How to read papers"},{"location":"howto-review-literature/#how-to-review-the-literature","text":"Here is Dylan's notes from the discussions in the Genesis story understanding group. Triage with abstracts: If the paper seems like a merely incremental change on a topic you already know, you can skip it. You can practice making these snap assessments. Read authors, not papers: Focus on understanding the work of particular researchers in general so you can contextualize all of their papers without reading all of their papers. Breadth-first citation: Starting with a few stand-out papers, recursively follow the interesting references in breadth-first order. Ask an expert: Don't try to read everything. Ask someone who's read everything what you should read. Rely on networks of trusted people to highlight good papers. Read using effective-writing rules: For example, you can use VSNC to parse an abstract or paper. Alternate absorbing and synthesizing: Take time in between bouts of reading to restate what you've read, draw connections, imagine applications, and dream up new possibilities. Reading for understanding is a constructive process. 7a. Note what others think is relevant: When others suggest related work, they're partly telling you what they think your work is. If they're off-base, it's an opportunity to clarify your own area. (Pay attention to misunderstandings). 7b. Get past \"Isn't this just X?\": One particularly reductive form of misunderstanding is \"Isn't your work just [topic]?\". It isn't really actionable, as it's the consequence of a listener who has simply stopped too early. Tells you more about the listener than about how to present your work.","title":"How to review the literature"},{"location":"literature-POMDP/","text":"","title":"literature POMDP"},{"location":"literature-irrationality/","text":"","title":"Literature irrationality"},{"location":"literature-planning/","text":"","title":"Literature planning"},{"location":"literature-weekly/","text":"Weekly readings # Week 5-7: # For Anna and Isabelle # Adaptive integration of habits into depth-limited planning defines a habitual-goal\u2013directed spectrum A relevant paper on how human planning combines goal-directed calculation and habitual estimation, and how the combination is changed when given time pressure. I think Isabelle's implementation may incorporate the notion of \"depth of planning\" and Anna's implementation may incorporate the notion of \"habitual values.\" For Darren # Planning and Explanations with a Learned Spatial Model A paper relevant to your recommendation system, especially Section 2 and 3 \"SemaFORR\u2019s spatial model is hierarchical, graph-oriented, and has well-defined semantics, all features observed in the models that people generate.\" \"SemaFORR demonstrates the power of a cognitive spatial model to inform both planning and user-friendly explanations, and to support autonomous navigation through the complexities of a large realistic world.\" A location representation for generating descriptive walking directions A paper relevant to the advise given to human rescuers. Instead of recommending \"Walk ten feet and turn right,\" it's more intuitive to use landmarks such as \"Walk to the end of the hallway and turn right\" Week 3: Bayesian Inverse Reinforcement Learning (For Christian Only) # Nonparametric Bayesian Inverse Reinforcement Learning for Multiple Reward Functions . Note from Tejas: As you read the paper could you please annotate it or make notes elsewhere for us to understand which technical areas you might not be familiar with and provide you with extra reading resources? Feel free to reach at anytime with your questions. It might be useful to document all your queries and reading progress in an Overleaf document and share it with us so that I can answer them periodically. (Optional, not for discussing this Thursday) It might also be worth going over the following paper on Bayesian IRL: Bayesian Inverse Reinforcement Learning . In recent times deep learning has also been used for IRL. Here is an example of a paper where they apply IRL for planning using planetary rovers (with an application to some gridworld settings): Rover-IRL: Inverse Reinforcement Learning With Soft Value Iteration Networks for Planetary Rover Path Planning Week 2: Inverse Planning # Baker, Chris, Rebecca Saxe, and Joshua Tenenbaum. \"Goal inference as inverse planning.\" , 2007. Baker, Chris, Rebecca Saxe, and Joshua Tenenbaum. \"Bayesian theory of mind: Modeling joint belief-desire attribution.\" , 2011. Week 1: MDP, Dynamic Programming, Reinforcement Learning # Several resources to learn the same thing. You can choose one to follow and the other two to skim through: Wikipedia page on Markov Decision Process (Up to Section 3 Algorithms) Stanford lecture on MDP and Value Iteration: Lecture 7: Markov Decision Processes - Value Iteration | Stanford CS221: AI (Autumn 2019) . Blog on Introduction to Reinforcement Learning , which is based on David Silver's lectures on reinforcement learning, Prerequisites # We've assumed that you know: Python: able to read and write with the help of Google/ Slack Overflow Object oriented programming: able to define a class with associated variables and functions in Python Data structures: know about list, dictionary git management: know how to add, commit, push, and revert","title":"Weekly readings"},{"location":"literature-weekly/#weekly-readings","text":"","title":"Weekly readings"},{"location":"literature-weekly/#week-5-7","text":"","title":"Week 5-7:"},{"location":"literature-weekly/#for-anna-and-isabelle","text":"Adaptive integration of habits into depth-limited planning defines a habitual-goal\u2013directed spectrum A relevant paper on how human planning combines goal-directed calculation and habitual estimation, and how the combination is changed when given time pressure. I think Isabelle's implementation may incorporate the notion of \"depth of planning\" and Anna's implementation may incorporate the notion of \"habitual values.\"","title":"For Anna and Isabelle"},{"location":"literature-weekly/#for-darren","text":"Planning and Explanations with a Learned Spatial Model A paper relevant to your recommendation system, especially Section 2 and 3 \"SemaFORR\u2019s spatial model is hierarchical, graph-oriented, and has well-defined semantics, all features observed in the models that people generate.\" \"SemaFORR demonstrates the power of a cognitive spatial model to inform both planning and user-friendly explanations, and to support autonomous navigation through the complexities of a large realistic world.\" A location representation for generating descriptive walking directions A paper relevant to the advise given to human rescuers. Instead of recommending \"Walk ten feet and turn right,\" it's more intuitive to use landmarks such as \"Walk to the end of the hallway and turn right\"","title":"For Darren"},{"location":"literature-weekly/#week-3-bayesian-inverse-reinforcement-learning-for-christian-only","text":"Nonparametric Bayesian Inverse Reinforcement Learning for Multiple Reward Functions . Note from Tejas: As you read the paper could you please annotate it or make notes elsewhere for us to understand which technical areas you might not be familiar with and provide you with extra reading resources? Feel free to reach at anytime with your questions. It might be useful to document all your queries and reading progress in an Overleaf document and share it with us so that I can answer them periodically. (Optional, not for discussing this Thursday) It might also be worth going over the following paper on Bayesian IRL: Bayesian Inverse Reinforcement Learning . In recent times deep learning has also been used for IRL. Here is an example of a paper where they apply IRL for planning using planetary rovers (with an application to some gridworld settings): Rover-IRL: Inverse Reinforcement Learning With Soft Value Iteration Networks for Planetary Rover Path Planning","title":"Week 3: Bayesian Inverse Reinforcement Learning (For Christian Only)"},{"location":"literature-weekly/#week-2-inverse-planning","text":"Baker, Chris, Rebecca Saxe, and Joshua Tenenbaum. \"Goal inference as inverse planning.\" , 2007. Baker, Chris, Rebecca Saxe, and Joshua Tenenbaum. \"Bayesian theory of mind: Modeling joint belief-desire attribution.\" , 2011.","title":"Week 2: Inverse Planning"},{"location":"literature-weekly/#week-1-mdp-dynamic-programming-reinforcement-learning","text":"Several resources to learn the same thing. You can choose one to follow and the other two to skim through: Wikipedia page on Markov Decision Process (Up to Section 3 Algorithms) Stanford lecture on MDP and Value Iteration: Lecture 7: Markov Decision Processes - Value Iteration | Stanford CS221: AI (Autumn 2019) . Blog on Introduction to Reinforcement Learning , which is based on David Silver's lectures on reinforcement learning,","title":"Week 1: MDP, Dynamic Programming, Reinforcement Learning"},{"location":"literature-weekly/#prerequisites","text":"We've assumed that you know: Python: able to read and write with the help of Google/ Slack Overflow Object oriented programming: able to define a class with associated variables and functions in Python Data structures: know about list, dictionary git management: know how to add, commit, push, and revert","title":"Prerequisites"},{"location":"notes-DARREN_LIM/","text":"Prediction Analysis # Overall, I will use past player actions to try to predict what players will do next; essentially I am looking for patterns in their actions. When they change their behavior, I will consider that a change in their action-making. For example, player 6 initially digs ender chests, so I will assume they will not stop digging it. However, after test 2 they stop digging it, and my justification is because they believe it takes too long. Therefore, I will now predict that they will not dig ender chests. Below I state my overall prediction strategies for each test. Player 6 # Test 2 # When did not know player well, guess As player did actions, learned what they would do so predicted they would repeat past actions Keep in mind they are searching for chests (the reward), so they will prioritize finding chests; this helps justify actions done They usually tended to go to the left. This was first a guess then it was made into theory as time went on based on past actions They dug ender chests always when they saw it They dug regular chests always when they saw it They almost always dug dirt, but if the dirt was far away around the corner and there was a closer door, they went through the door Test 3 # Predict based on the patterns learned from the past experiment, keeping in mind they may change their patterns in a different test It seems that they are now skipping ender chests because it takes too long They still tend to go to left They usually dig dirt when they see it closeby, unless they don't have much time Test 5 # Predict based on the patterns learned from the past experiment, keeping in mind they may change their patterns in a different test They were typically consistent with before actions Sometimes they did things differently for unknown reasons They seem to have a love-hate relationship with dirt Player 10 # Test 2 # They dig chests when they see it (regardless of type) They seem to like digging dirt and prioritize it They do not seem spatially aware and can get lost They prefer left actions when faced with directions Test 3 # Predict based on the patterns learned from the past experiment, keeping in mind they may change their patterns in a different test They have made a lot of unpredictable actions this round that differed than before. Regardless, we learned: They usually prefer doors to dirt when they are very close together They still dig ender chests Test 5 # Predict based on the patterns learned from the past experiment, keeping in mind they may change their patterns in a different test Both player 6 and player 10 don't like that door that has dirt in the back and tend to skip it. Maybe a sign of the map warning? Overall prediction considerations # Essentially, there seems to be a few aspects that predictors will keep in mind when trying to decide how a player will act. Past player data. If they have done something in the past, there is a good chance they will repeat it in a similar circumstance Reward function consideration. They will keep in mind that the goal is to dig chests for reward, so actions done will reflect this goal. Commonsense data: If they open a door, there is a good chance they will go through it If they walk towards a chest, there is a good chance they will dig it If they walk towards door/dirt, there is a good chance they will go through it or dig it If they seem not spatially aware, they may repeat doors. Otherwise, they won't Raw player prediction data # Below is my raw prediction data that shows my predictions for each video in every test, as well as a justification for why I believe they will do this particular action. Player 6 prediction data # Test 2 # Video Number Prediction Justification Correct? 1 Go left The player didn't see what in the hallway, so they will look around. Also, don't know this player yet so can't say for sure YES 2 Go through the door on the left Closest door, and there are no chests in sight, but other doors are in sight. YES 3 Go left The player went left last time. Not sure otherwise YES 4 Dig the dirt The player has never seen dirt before, and I think they will dig it to see what is behind it YES 5 Go left We don't know what is in the hole on the left, so the player may look at it if it\u2019s a door YES 6 Dig the ender chest See chest, dig it YES 7 Dig dirt They did it last time and since there was a chest behind the first one they might check again YES 8 Go through the door in front It's either the door or the hallway that seems to go down farther away. Door is closer. YES 9 Dig chest See chest, dig it YES 10 Go to left They usually go left when faced with multiple directions to go. Not too certain here. YES*** (they looked at the left but the video ended before they made a decision) 11 Go to dirt on right It seems that they were moving to the right, so it looks like they will return to the chest they saw NO, they moved back to the chest 12 Go through door It's closer, and last time there was nothing behind the dirt so it might have discouraged them from going to the dirt YES 13 Look at the doors at the end of the hallway since that's where they're facing They're facing that way so they might as well look NO they turned around the moved away 14 Go through the door that was closed on the right (closest to them) It's unopened and they usually go through doors unopened YES 15 Go into door They just opened it YES Test 3 # Video Number Prediction Justification Correct? 1 Dig dirt The door leads to more places, and the dirt is close by. YES 2 Dig chest The chest is super close and they always had dug chests when they saw them YES 3 Dig dirt They dug the dirt when there were doors nearby before NO, they went through the door in front (maybe discouraged last time when there wasn't a chest behind the dirt) 4 Go through the door Always had opened door then gone through door NO, they went back to the dirt (maybe they changed their mind about skipping the dirt and dug it anyways) 5 Walk behind dirt to check for chests They always check for chests after digging dirt YES 6 Go through the door that was opened They opened it, and all other doors are closed YES 7 Uncertain. Dig dirt? Last time when confronted with doors and dirt they opened door then dug dirt, but this time they might just dig dirt? YES 8 Door on left They usually aim to left YES 9 Keep digging Uncertain. They never saw this situation before, so possibly they are curious to see what is behind the dirt. YES 10 Stop digging There's too much dirt and they saw an opening with nothing there. No reward, leave YES 11 Open door in front It's unopened and they haven't gone in that direction yet YES 12 Dig ender chest No evidence thus far says they will skip ender chests NO. It seems they have changed their strategy to skip ender chests, since they take too long 13 Dig chest See regular chest, dig it YES 14 Dig dirt It's close and they usually dig it NO they skipped it. Perhaps they are time constrained and realize dirt takes too long 15 Go through door in front It's the only unopened one in view YES Test 5 # Video Number Prediction Justification Correct? 1 Go to door that they saw Nowhere else looked promising YES 2 Go in door They opened it so they always went through it NO. Behavior change: dirt deterred them more than before, so now they will skip it cause they saw dirt 3 Open door They are looking at it YES 4 Go through the door they skipped It's closeby and they haven't yet explored it YES 5 Open door They are looking at it and it's close to them YES 6 Go to left where they haven't explored yet It's an open hallway that is not explored yet NO they return backwards, seem uncertain, then go back again 7 Go left They usually go left when not knowing where to go NO, they went forward. (this might've been cause they knew the map and I am not referencing it) 8 Open door They are looking at it YES 9 Open door They are looking at it YES 10 Skip the ender chest and go backwards They didn't dig the ender chest last time YES 11 Open door They are looking at it NO, they went around and opened a different door 12 Dig dirt They usually dig dirt when they see it YES 13 Continue digging dirt They haven't really dug 2 layers yet YES 14 Go to right They haven't looked in that direction yet in the room NO, they left the room. Possibly time constrained 15 Go to right They saw a chest there before YES Player 10 prediction data # Test 2 # Video Number Prediction Justification Correct? 1 Open door in front Just a guess. They saw door, went in front. YES 2 Look left for chests They see dirt on right, but nothing seen on left yet YES 3 Dig dirt They are walking towards it and they haven't dug it before YES 4 Go left They saw more stuff on the left (saw the door) YES 5 Go left door They tend to go left YES 6 Dig ender chest They haven't seen ender chests before and will probably dig it YES 7 Dig dirt They are looking right at it and walking towards it YES 8 Go through door in front They see a door and nothing else around them yet YES 9 Dig chest in front See chest, dig it YES 10 Look left at hallway they haven't seen what is on their left yet NO, they walked to the dirt 11 Dig the dirt They walked to the dirt instead of looking left so it seems they are aiming for it YES 12 Go left They haven't looked left yet YES 13 Dig ender chest They have not shown signs of not digging ender chests yet YES 14 Exit the door in front They are walking towards it YES 15 Look right They already looked to the left. IF THEY ARE NOT SPATIALLY AWARE THEY MAY RETURN LEFT NO they looked left instead 16 Dig the ender chest they saw It's the closest thing to them that they can dig YES 17 Walk down the hallway and turn right They haven't gone there yet, all other doors are opened YES 18 It seems they are not spatially aware, so they may go through that closest door even though it is a visited room (also they didn't fully explore the visited room) See above YES Test 3 # Video Number Prediction Justification Correct? 1 Dig the dirt in front They seem to enjoy digging dirt from last time YES 2 Dig the dirt behind them Same reason as test 1 NO, they went through the door; it was closer I guess? 3 Dig the dirt in front of them Same reason as test 1 YES 4 Dig the dirt Same reason as test 1 NO, they don't like that dirt (maybe because it shows that there is a door behind it) 5 Go through door They opened it YES 6 Dig the dirt Test 1 reason NO, they went through the door instead. It seems that if there is a door and dirt very close by, they choose door. 7 Dig ender chest No signs otherwise saying they won't YES 8 Dig dirt on left Dig the dirt before leaving through the last door NO, I should've known they would skip the dirt like last time rip 9 Go through door They opened it NO, they changed their mind and dug the dirt finally 10 Go through the door they opened they opened it YES 11 Dig ender chest They saw it and always have dug it before YES Test 5 # Video Number Prediction Justification Correct? 1 Go left They tend to go left when given multiple decisions YES 2 Open the door They were walking towards it YES 3 Go left, door closest to them They tend to go left YES 4 Open the door They were walking towards it YES 5 Dig ender chest No reason to think they will stop, they haven't before YES 6 Dig the dirt on their left There is no door and they seem to enjoy digging dirt YES 7 Go left They usually go left NO, they went through the door that was closer that they saw 8 Go through door They opened it NO they turned around and went to the door behind instead 9 Go through door they opened It is open and easier to go through NO they looked right 10 Go through door they opened See reason 9 NO they skipped it then returned to where they were before, possibly another sign that they are not spatially aware 11 Return to the door unopened See reason 9 NO they hate that door 12 Dig the dirt in front of them They went through to the room and saw dirt YES","title":"Prediction Analysis"},{"location":"notes-DARREN_LIM/#prediction-analysis","text":"Overall, I will use past player actions to try to predict what players will do next; essentially I am looking for patterns in their actions. When they change their behavior, I will consider that a change in their action-making. For example, player 6 initially digs ender chests, so I will assume they will not stop digging it. However, after test 2 they stop digging it, and my justification is because they believe it takes too long. Therefore, I will now predict that they will not dig ender chests. Below I state my overall prediction strategies for each test.","title":"Prediction Analysis"},{"location":"notes-DARREN_LIM/#player-6","text":"","title":"Player 6"},{"location":"notes-DARREN_LIM/#test-2","text":"When did not know player well, guess As player did actions, learned what they would do so predicted they would repeat past actions Keep in mind they are searching for chests (the reward), so they will prioritize finding chests; this helps justify actions done They usually tended to go to the left. This was first a guess then it was made into theory as time went on based on past actions They dug ender chests always when they saw it They dug regular chests always when they saw it They almost always dug dirt, but if the dirt was far away around the corner and there was a closer door, they went through the door","title":"Test 2"},{"location":"notes-DARREN_LIM/#test-3","text":"Predict based on the patterns learned from the past experiment, keeping in mind they may change their patterns in a different test It seems that they are now skipping ender chests because it takes too long They still tend to go to left They usually dig dirt when they see it closeby, unless they don't have much time","title":"Test 3"},{"location":"notes-DARREN_LIM/#test-5","text":"Predict based on the patterns learned from the past experiment, keeping in mind they may change their patterns in a different test They were typically consistent with before actions Sometimes they did things differently for unknown reasons They seem to have a love-hate relationship with dirt","title":"Test 5"},{"location":"notes-DARREN_LIM/#player-10","text":"","title":"Player 10"},{"location":"notes-DARREN_LIM/#test-2_1","text":"They dig chests when they see it (regardless of type) They seem to like digging dirt and prioritize it They do not seem spatially aware and can get lost They prefer left actions when faced with directions","title":"Test 2"},{"location":"notes-DARREN_LIM/#test-3_1","text":"Predict based on the patterns learned from the past experiment, keeping in mind they may change their patterns in a different test They have made a lot of unpredictable actions this round that differed than before. Regardless, we learned: They usually prefer doors to dirt when they are very close together They still dig ender chests","title":"Test 3"},{"location":"notes-DARREN_LIM/#test-5_1","text":"Predict based on the patterns learned from the past experiment, keeping in mind they may change their patterns in a different test Both player 6 and player 10 don't like that door that has dirt in the back and tend to skip it. Maybe a sign of the map warning?","title":"Test 5"},{"location":"notes-DARREN_LIM/#overall-prediction-considerations","text":"Essentially, there seems to be a few aspects that predictors will keep in mind when trying to decide how a player will act. Past player data. If they have done something in the past, there is a good chance they will repeat it in a similar circumstance Reward function consideration. They will keep in mind that the goal is to dig chests for reward, so actions done will reflect this goal. Commonsense data: If they open a door, there is a good chance they will go through it If they walk towards a chest, there is a good chance they will dig it If they walk towards door/dirt, there is a good chance they will go through it or dig it If they seem not spatially aware, they may repeat doors. Otherwise, they won't","title":"Overall prediction considerations"},{"location":"notes-DARREN_LIM/#raw-player-prediction-data","text":"Below is my raw prediction data that shows my predictions for each video in every test, as well as a justification for why I believe they will do this particular action.","title":"Raw player prediction data"},{"location":"notes-DARREN_LIM/#player-6-prediction-data","text":"","title":"Player 6 prediction data"},{"location":"notes-DARREN_LIM/#test-2_2","text":"Video Number Prediction Justification Correct? 1 Go left The player didn't see what in the hallway, so they will look around. Also, don't know this player yet so can't say for sure YES 2 Go through the door on the left Closest door, and there are no chests in sight, but other doors are in sight. YES 3 Go left The player went left last time. Not sure otherwise YES 4 Dig the dirt The player has never seen dirt before, and I think they will dig it to see what is behind it YES 5 Go left We don't know what is in the hole on the left, so the player may look at it if it\u2019s a door YES 6 Dig the ender chest See chest, dig it YES 7 Dig dirt They did it last time and since there was a chest behind the first one they might check again YES 8 Go through the door in front It's either the door or the hallway that seems to go down farther away. Door is closer. YES 9 Dig chest See chest, dig it YES 10 Go to left They usually go left when faced with multiple directions to go. Not too certain here. YES*** (they looked at the left but the video ended before they made a decision) 11 Go to dirt on right It seems that they were moving to the right, so it looks like they will return to the chest they saw NO, they moved back to the chest 12 Go through door It's closer, and last time there was nothing behind the dirt so it might have discouraged them from going to the dirt YES 13 Look at the doors at the end of the hallway since that's where they're facing They're facing that way so they might as well look NO they turned around the moved away 14 Go through the door that was closed on the right (closest to them) It's unopened and they usually go through doors unopened YES 15 Go into door They just opened it YES","title":"Test 2"},{"location":"notes-DARREN_LIM/#test-3_2","text":"Video Number Prediction Justification Correct? 1 Dig dirt The door leads to more places, and the dirt is close by. YES 2 Dig chest The chest is super close and they always had dug chests when they saw them YES 3 Dig dirt They dug the dirt when there were doors nearby before NO, they went through the door in front (maybe discouraged last time when there wasn't a chest behind the dirt) 4 Go through the door Always had opened door then gone through door NO, they went back to the dirt (maybe they changed their mind about skipping the dirt and dug it anyways) 5 Walk behind dirt to check for chests They always check for chests after digging dirt YES 6 Go through the door that was opened They opened it, and all other doors are closed YES 7 Uncertain. Dig dirt? Last time when confronted with doors and dirt they opened door then dug dirt, but this time they might just dig dirt? YES 8 Door on left They usually aim to left YES 9 Keep digging Uncertain. They never saw this situation before, so possibly they are curious to see what is behind the dirt. YES 10 Stop digging There's too much dirt and they saw an opening with nothing there. No reward, leave YES 11 Open door in front It's unopened and they haven't gone in that direction yet YES 12 Dig ender chest No evidence thus far says they will skip ender chests NO. It seems they have changed their strategy to skip ender chests, since they take too long 13 Dig chest See regular chest, dig it YES 14 Dig dirt It's close and they usually dig it NO they skipped it. Perhaps they are time constrained and realize dirt takes too long 15 Go through door in front It's the only unopened one in view YES","title":"Test 3"},{"location":"notes-DARREN_LIM/#test-5_2","text":"Video Number Prediction Justification Correct? 1 Go to door that they saw Nowhere else looked promising YES 2 Go in door They opened it so they always went through it NO. Behavior change: dirt deterred them more than before, so now they will skip it cause they saw dirt 3 Open door They are looking at it YES 4 Go through the door they skipped It's closeby and they haven't yet explored it YES 5 Open door They are looking at it and it's close to them YES 6 Go to left where they haven't explored yet It's an open hallway that is not explored yet NO they return backwards, seem uncertain, then go back again 7 Go left They usually go left when not knowing where to go NO, they went forward. (this might've been cause they knew the map and I am not referencing it) 8 Open door They are looking at it YES 9 Open door They are looking at it YES 10 Skip the ender chest and go backwards They didn't dig the ender chest last time YES 11 Open door They are looking at it NO, they went around and opened a different door 12 Dig dirt They usually dig dirt when they see it YES 13 Continue digging dirt They haven't really dug 2 layers yet YES 14 Go to right They haven't looked in that direction yet in the room NO, they left the room. Possibly time constrained 15 Go to right They saw a chest there before YES","title":"Test 5"},{"location":"notes-DARREN_LIM/#player-10-prediction-data","text":"","title":"Player 10 prediction data"},{"location":"notes-DARREN_LIM/#test-2_3","text":"Video Number Prediction Justification Correct? 1 Open door in front Just a guess. They saw door, went in front. YES 2 Look left for chests They see dirt on right, but nothing seen on left yet YES 3 Dig dirt They are walking towards it and they haven't dug it before YES 4 Go left They saw more stuff on the left (saw the door) YES 5 Go left door They tend to go left YES 6 Dig ender chest They haven't seen ender chests before and will probably dig it YES 7 Dig dirt They are looking right at it and walking towards it YES 8 Go through door in front They see a door and nothing else around them yet YES 9 Dig chest in front See chest, dig it YES 10 Look left at hallway they haven't seen what is on their left yet NO, they walked to the dirt 11 Dig the dirt They walked to the dirt instead of looking left so it seems they are aiming for it YES 12 Go left They haven't looked left yet YES 13 Dig ender chest They have not shown signs of not digging ender chests yet YES 14 Exit the door in front They are walking towards it YES 15 Look right They already looked to the left. IF THEY ARE NOT SPATIALLY AWARE THEY MAY RETURN LEFT NO they looked left instead 16 Dig the ender chest they saw It's the closest thing to them that they can dig YES 17 Walk down the hallway and turn right They haven't gone there yet, all other doors are opened YES 18 It seems they are not spatially aware, so they may go through that closest door even though it is a visited room (also they didn't fully explore the visited room) See above YES","title":"Test 2"},{"location":"notes-DARREN_LIM/#test-3_3","text":"Video Number Prediction Justification Correct? 1 Dig the dirt in front They seem to enjoy digging dirt from last time YES 2 Dig the dirt behind them Same reason as test 1 NO, they went through the door; it was closer I guess? 3 Dig the dirt in front of them Same reason as test 1 YES 4 Dig the dirt Same reason as test 1 NO, they don't like that dirt (maybe because it shows that there is a door behind it) 5 Go through door They opened it YES 6 Dig the dirt Test 1 reason NO, they went through the door instead. It seems that if there is a door and dirt very close by, they choose door. 7 Dig ender chest No signs otherwise saying they won't YES 8 Dig dirt on left Dig the dirt before leaving through the last door NO, I should've known they would skip the dirt like last time rip 9 Go through door They opened it NO, they changed their mind and dug the dirt finally 10 Go through the door they opened they opened it YES 11 Dig ender chest They saw it and always have dug it before YES","title":"Test 3"},{"location":"notes-DARREN_LIM/#test-5_3","text":"Video Number Prediction Justification Correct? 1 Go left They tend to go left when given multiple decisions YES 2 Open the door They were walking towards it YES 3 Go left, door closest to them They tend to go left YES 4 Open the door They were walking towards it YES 5 Dig ender chest No reason to think they will stop, they haven't before YES 6 Dig the dirt on their left There is no door and they seem to enjoy digging dirt YES 7 Go left They usually go left NO, they went through the door that was closer that they saw 8 Go through door They opened it NO they turned around and went to the door behind instead 9 Go through door they opened It is open and easier to go through NO they looked right 10 Go through door they opened See reason 9 NO they skipped it then returned to where they were before, possibly another sign that they are not spatially aware 11 Return to the door unopened See reason 9 NO they hate that door 12 Dig the dirt in front of them They went through to the room and saw dirt YES","title":"Test 5"},{"location":"notes_Anna/","text":"My predictions # My strategies # Left or Right: - When player is at left of a corridor, I predict he will turn left; when player is at right of a corridor, I predict player will turn right - In rooms, player would explore left side first when player can see more things on the left and explore right side first when player can see more things on the right - When I don't know player will turn left or right, I always choose left Exploration preferences: - At beginning of a game, player tend to explore smaller spaces first - Players always want to explore new area (open doors, clear up dirt) at the beginning - When player already sees a door/dirt, player tend to directly explore rather than look arround and then explore Picking up chests: - When player hasn't given up any green chests before, always assume they will pick up green chest when they see one - When player begin to give up green chest, always assume they will give up green chest if not at the end game End games: - At end games, player would tend to focus on one mission - Assume player would take any immediate reward - Go to the nearest unexplored area Other strategies: - when player done sth, he/she is likely to do that again - when player already see a newly exposed door/dirt but hesitated, player probably will not choose to explore that door/dirt My results # Bad at predicting which house player will go into at map 5 (too many seemingly equivalent choices) Bad at predicting player 6's end game actions Otherwise pretty good Observers: # For all observers in general # Player's behavior tend to change at end games, or become less rational and regulized, thus it is hard for observers to predict player actions in this case It is hard for observers to perdict which door/dirt player will choose to explore when there are multiple totally new doors and/or dirts in sight. Notice that prediction accuracy is very low (for both player6 and player10) on middle to end part of maze 5. This is because it is basically asking 'which new door/dirt/direction do you think player will explore' without giving observers much relevant information. Observers always think rationally, but players sometime don't, so observers tend to make wrong predictions on the same set of questions (when player is not behaving as rationally). Different types of observers # Some tend to analyze player's overall strategy throughout the game and apply their observations to later maze's predictions, e.g. player's room search sequence & player's behavior difference in first and second half of the game Some tend to notice player's preference on local rewards, e.g. door or dirt, left or right, rather than player's overall strategy. Some observer immediately change their prediction strategy after making wrong predictions and notice the player is thinking in a different way, some will stick to their choices for a few more rounds before they switch strategy. Some observers always take the player as a rational agent, so they can make good predictions when the player is acting rational. But those observers will predict very wrong when the player is acting rather random Comparatively, some observers will make random predictions or avoid the most 'rational' choice when they notice the player might not be acting systematically, so their prediction accuracy is not too low when players are acting randomly.","title":"My predictions"},{"location":"notes_Anna/#my-predictions","text":"","title":"My predictions"},{"location":"notes_Anna/#my-strategies","text":"Left or Right: - When player is at left of a corridor, I predict he will turn left; when player is at right of a corridor, I predict player will turn right - In rooms, player would explore left side first when player can see more things on the left and explore right side first when player can see more things on the right - When I don't know player will turn left or right, I always choose left Exploration preferences: - At beginning of a game, player tend to explore smaller spaces first - Players always want to explore new area (open doors, clear up dirt) at the beginning - When player already sees a door/dirt, player tend to directly explore rather than look arround and then explore Picking up chests: - When player hasn't given up any green chests before, always assume they will pick up green chest when they see one - When player begin to give up green chest, always assume they will give up green chest if not at the end game End games: - At end games, player would tend to focus on one mission - Assume player would take any immediate reward - Go to the nearest unexplored area Other strategies: - when player done sth, he/she is likely to do that again - when player already see a newly exposed door/dirt but hesitated, player probably will not choose to explore that door/dirt","title":"My strategies"},{"location":"notes_Anna/#my-results","text":"Bad at predicting which house player will go into at map 5 (too many seemingly equivalent choices) Bad at predicting player 6's end game actions Otherwise pretty good","title":"My results"},{"location":"notes_Anna/#observers","text":"","title":"Observers:"},{"location":"notes_Anna/#for-all-observers-in-general","text":"Player's behavior tend to change at end games, or become less rational and regulized, thus it is hard for observers to predict player actions in this case It is hard for observers to perdict which door/dirt player will choose to explore when there are multiple totally new doors and/or dirts in sight. Notice that prediction accuracy is very low (for both player6 and player10) on middle to end part of maze 5. This is because it is basically asking 'which new door/dirt/direction do you think player will explore' without giving observers much relevant information. Observers always think rationally, but players sometime don't, so observers tend to make wrong predictions on the same set of questions (when player is not behaving as rationally).","title":"For all observers in general"},{"location":"notes_Anna/#different-types-of-observers","text":"Some tend to analyze player's overall strategy throughout the game and apply their observations to later maze's predictions, e.g. player's room search sequence & player's behavior difference in first and second half of the game Some tend to notice player's preference on local rewards, e.g. door or dirt, left or right, rather than player's overall strategy. Some observer immediately change their prediction strategy after making wrong predictions and notice the player is thinking in a different way, some will stick to their choices for a few more rounds before they switch strategy. Some observers always take the player as a rational agent, so they can make good predictions when the player is acting rational. But those observers will predict very wrong when the player is acting rather random Comparatively, some observers will make random predictions or avoid the most 'rational' choice when they notice the player might not be acting systematically, so their prediction accuracy is not too low when players are acting randomly.","title":"Different types of observers"},{"location":"project-SAR/","text":"Search and Rescue (SAR) missions # I didn't really understand how essential teamwork is to SAR until I played a Minecraft team survival game. Lost in abandoned Mineshafts, killed around spawn points, forever lack of food supplies ... thinking about how a team coacher might advise me has made me a better team player. -- Yang Real-world SAR missions # SAR: Search for and provision of aid to people who are in distress or imminent danger. Real-world SAR tasks usually include the following features: Collapsed environment in urban SAR Natural environment in ground, mountain, cave, and maritime SAR Perceptually disorienting Physically and cognitively fatiguing Operations of hazard materials or weapons of mass destruction SAR team and procedure usually consists of: Further readings: Jennifer L. Burke, Robin R. Murphy, Michael D. Coovert, and Dawn L. Riddle, Moonlight in Miami: A Field Study of Human\u2013Robot Interaction in the Context of an Urban Search and Rescue Disaster Response Training Exercise , 2004 Yuening Zhang, slides on USAR practice","title":"Task - Search and rescue missions"},{"location":"project-SAR/#search-and-rescue-sar-missions","text":"I didn't really understand how essential teamwork is to SAR until I played a Minecraft team survival game. Lost in abandoned Mineshafts, killed around spawn points, forever lack of food supplies ... thinking about how a team coacher might advise me has made me a better team player. -- Yang","title":"Search and Rescue (SAR) missions"},{"location":"project-SAR/#real-world-sar-missions","text":"SAR: Search for and provision of aid to people who are in distress or imminent danger. Real-world SAR tasks usually include the following features: Collapsed environment in urban SAR Natural environment in ground, mountain, cave, and maritime SAR Perceptually disorienting Physically and cognitively fatiguing Operations of hazard materials or weapons of mass destruction SAR team and procedure usually consists of: Further readings: Jennifer L. Burke, Robin R. Murphy, Michael D. Coovert, and Dawn L. Riddle, Moonlight in Miami: A Field Study of Human\u2013Robot Interaction in the Context of an Urban Search and Rescue Disaster Response Training Exercise , 2004 Yuening Zhang, slides on USAR practice","title":"Real-world SAR missions"},{"location":"project-behavior/","text":"Human behavior in Minecraft SAR missions # In week 2 and 3, we will focus on understanding human behavior. Note that we have a different plan for Anna/Isabelle and for Christian/Darren in Week 2. For Anna and Isabelle, you will focus on watching videos from both Pilot test 1 and Pilot test 2. For Christian and Darren, you will focus on watching videos from ONLY Pilot test 1, until you have finished participating in ASU Pilot test. That's because the experiment design of our Pilot test 2 may spoil your ASU Pilot experience. In week 3, we will move onto Pilot test 2 and 3. Pilot test 1: Maze of size 45 by 46, player # In Pilot test 1, we want to see how human players behave in a SAR mission given two different goals: (1) triage all victims in the maze, (2) maximize the number of victims triaged within 10 minutes. Victims with different colors take different amount of time. From blue, green, yellow, orange, to red, they take 2, 4, 6, 8, and 10 seconds. Players are allowed to turn on lights, clear blockage, and put out fire. Resources for your analysis: Videos of 10 players searching the maze while thinking aloud: Dropbox link Minecraft world file - Singleplayer: Dropbox link Please only look at the following example analysis after you've generated your own theories and made your own summaries: Example analysis of player 1-5 behavior: Google slides Example analysis of player 6-10 behavior: Google slides Example lessons learned from player 1-10 behavior: Google slides Pilot test 2: Maze of size 24 by 24, player # For Christian and Darren, please stop reading further until have finished participating in ASU Pilot test. The experiment design of our Pilot test 2 may spoil your ASU Pilot experience. In Pilot test 2, we want to see how human players behave in SAR missions that have tight time constraints and different initial training conditions. The time limit of each player depends on his game expertise and ranges from 2-3 min. Each player will go through all three training conditions as described below: Resources for your analysis: Videos of 14 players searching five mazes of size 24 by 24: Dropbox link Maps of the five test mazes: Dropbox link (includes both the wall-only version presented to the players for planning beforehand and the actual maps) Summary of player time limit, game competency, and scores: Google slides Summary of player reported strategies and preferences: Google slides Download human trajectory dataset (visualized traces on maps): Dropbox link (2.28 MB) Description of the human trajectory dataset: Dataset I - Human trajectory Please only look at the following example analysis after you've generated your own theories and made your own summaries: Analysis of player behaviors: Google slides Categorization of player behaviors: Google slides Pilot test 3: Maze of size 24 by 24, observer # In Pilot test 3, we want to see how good are humans in predicting actions? Video clips of 2 players searching three mazes of size 24 by 24: Player 6 , Player 10 , Raw data of prediction questions, answers, and commentary: Google sheet Analysis of observer predictions: Google slides","title":"(New!) Human behavior"},{"location":"project-behavior/#human-behavior-in-minecraft-sar-missions","text":"In week 2 and 3, we will focus on understanding human behavior. Note that we have a different plan for Anna/Isabelle and for Christian/Darren in Week 2. For Anna and Isabelle, you will focus on watching videos from both Pilot test 1 and Pilot test 2. For Christian and Darren, you will focus on watching videos from ONLY Pilot test 1, until you have finished participating in ASU Pilot test. That's because the experiment design of our Pilot test 2 may spoil your ASU Pilot experience. In week 3, we will move onto Pilot test 2 and 3.","title":"Human behavior in Minecraft SAR missions"},{"location":"project-behavior/#pilot-test-1-maze-of-size-45-by-46-player","text":"In Pilot test 1, we want to see how human players behave in a SAR mission given two different goals: (1) triage all victims in the maze, (2) maximize the number of victims triaged within 10 minutes. Victims with different colors take different amount of time. From blue, green, yellow, orange, to red, they take 2, 4, 6, 8, and 10 seconds. Players are allowed to turn on lights, clear blockage, and put out fire. Resources for your analysis: Videos of 10 players searching the maze while thinking aloud: Dropbox link Minecraft world file - Singleplayer: Dropbox link Please only look at the following example analysis after you've generated your own theories and made your own summaries: Example analysis of player 1-5 behavior: Google slides Example analysis of player 6-10 behavior: Google slides Example lessons learned from player 1-10 behavior: Google slides","title":"Pilot test 1: Maze of size 45 by 46, player"},{"location":"project-behavior/#pilot-test-2-maze-of-size-24-by-24-player","text":"For Christian and Darren, please stop reading further until have finished participating in ASU Pilot test. The experiment design of our Pilot test 2 may spoil your ASU Pilot experience. In Pilot test 2, we want to see how human players behave in SAR missions that have tight time constraints and different initial training conditions. The time limit of each player depends on his game expertise and ranges from 2-3 min. Each player will go through all three training conditions as described below: Resources for your analysis: Videos of 14 players searching five mazes of size 24 by 24: Dropbox link Maps of the five test mazes: Dropbox link (includes both the wall-only version presented to the players for planning beforehand and the actual maps) Summary of player time limit, game competency, and scores: Google slides Summary of player reported strategies and preferences: Google slides Download human trajectory dataset (visualized traces on maps): Dropbox link (2.28 MB) Description of the human trajectory dataset: Dataset I - Human trajectory Please only look at the following example analysis after you've generated your own theories and made your own summaries: Analysis of player behaviors: Google slides Categorization of player behaviors: Google slides","title":"Pilot test 2: Maze of size 24 by 24, player"},{"location":"project-behavior/#pilot-test-3-maze-of-size-24-by-24-observer","text":"In Pilot test 3, we want to see how good are humans in predicting actions? Video clips of 2 players searching three mazes of size 24 by 24: Player 6 , Player 10 , Raw data of prediction questions, answers, and commentary: Google sheet Analysis of observer predictions: Google slides","title":"Pilot test 3: Maze of size 24 by 24, observer"},{"location":"project-ideas/","text":"Project ideas for summer UROP students # Thoughts, in no particular order. Can personality survey result help agents make better action predictions? #","title":"Project ideas for summer UROP students"},{"location":"project-ideas/#project-ideas-for-summer-urop-students","text":"Thoughts, in no particular order.","title":"Project ideas for summer UROP students"},{"location":"project-ideas/#can-personality-survey-result-help-agents-make-better-action-predictions","text":"","title":"Can personality survey result help agents make better action predictions?"},{"location":"project-meetings/","text":"Weekly meeting minutes # Here are meeting minutes of each weekly meeting. The most recent weeks come first. Week 8 - July 20th # Darren: Start implementing his ideas Plan to incorporate Yuening\u2019s temporal framework for next week Problem he met: Notice that finding a route that get through all the rooms for the agent is a more complex problem then he thought Christian: Almost done with a function that could transfer our\u2019s mdp model to another mdp model Plan to work on generating reward function for agents in inverse planning framework Anna: * Implemented a memory system for agents, start to change how room values are assigned * Plan to continue exploring how to change room values so that agent can behave more \u2018human\u2019 Isabelle: * Tried a new algorithm, find out it didn\u2019t work * Plan to implement tile_level_value_iteration with a different algorithm Week 7 - July 13th # Slides from UROP presentations Darren's slides on recommendation system: https://docs.google.com/presentation/d/1L5oIImOmCtqgyPgfelrYWuMUcwp_I3hb9LdaqXi8WI8/edit?usp=sharing Christian's slides on Bayesian inverse reinforcement learning: https://docs.google.com/presentation/d/1-BopTio00nzo1_8fXMAHf2oQlSPB4PY6iqJ4VEIOssQ/edit Anna's: https://docs.google.com/presentation/d/1BhOze5k_UC4YG-7O_cK7oKi7tf3q8eSeCSTwzGJu5UI/edit?usp=sharing Isabelle's: https://drive.google.com/drive/folders/1oMdqFolh-2R4-Ev9NmybcmvKsqRjwMnH?usp=sharing Week 6 - July 6th (No meeting due to Independence Day holidays) # Week 5 - June 29th # Progress updates: Anna: Finished pilot test 3, gave observations and strategies Was looking into hierarchal code, found some problems and tried to fix Isabelle: Week 3 focused on how to model player behaviors observed Played around w parameters and reward functions Will post on slack channel Looked at hierarchal.py and ideas to implement her own branch Observed players (pilot test 3) as a participant, looked at results and wrote report, uploaded to git Christian: Did ASU test, conclusions are in meeting notes (maybe) Interesting that their victims died in the middle Strategy was suboptimal but didn\u2019t find out until failed at getting all of them Tejas provided lots of supplementary material Read Bayesian reinforcement paper Went over lecture notes from 6.437 Next step implementing bayesian learning Yang: Showed ASU data, will be useful Next Monday we will all give 15 min presentations on behaviors and planning 4-15 pages of slides to use videos/images to summarize plan Optional: give plan for computational model Yuening's presentation: Presentation on temporal networks In this case: temporal networks are using for mainly generative activity planning (or conditional planning, not being talked about) Mainly come up with sequence of actions to achieve plan Background on lab, MERS Goal: build robots that think for themselves and design how humans program cognitive robots ex: underwater rovers, mars rovers, autonomous vehicles Problem: failures occur Want to write high level programs that tell the robot what to achieve, then the robot can reason how to achieve this goal Ex: put out the fires at loc X and Y, then return in an hour. Avoid no fly zones. Here's a map. Programs are state based and model based Need to have planning and ability to execute plan Need to be aware of the risk and utility Simple temporal networks (STN) Scheduling representation We have events and temporal constraints (this occurs after that) This allows us to create flexible execution on the fly System can adapt: can have ranges of when events can occur based on temporal constraints How to detect inconsistencies: Two equalities: change constraint to two opposing directed edges (negate the one going away) Becomes a distance graph Simply check for negative cycles for inconsistencies (APSP) How to dispatch events Key idea: need to assign events in increasing order; once event is dispatched at time t, then you cannot dispatch anything before that anymore Negative edges from APSP dictate ordering constraints Ex: -1 negative edge means that the head needs to occur before the tail Then, propagate time constraint to other events and continue Simple temporal networks with uncertainty (STNU) Allows uncontrollable events Ex: cannot control when an event happens, but we know the delay between the previous event to the next event Three types of controllability: Weak, dynamic, and strong Dynamic: can generate schedule online given past uncontrollable durations Strong: generate schedule no matter how long the uncontrollables take Only if consistent or controllable can a schedule be found Example application: Uhura A user talks about their schedule then the planner looks for if consistent/controllable; if not, ask them to relax some requirements Algorithm is called temporal relaxation If inconsistent, extract negative cycles to find out how to resolve these conflicts Need to change neg cycle to not neg cycle by either: Discrete or Continuous relaxation Discrete: just don't satisfy that constraint Continuous: make it non negative by weakening the constraint (lower/upper bounds) Look for least costly relaxation Also: looking at d-Uhura which works in teams When ppl collaborate in tasks, they are constantly overloaded It's an oversubscription problem with multiple users where changes can have a rippling event d-Uhura helps teams adapt their schedule together Locally, try to get individuals to fix schedule without affecting others If not, collaborate with others Extensions: Temporal networks to temporal plan network Introduce choice nodes: controllable by agent, or uncontrollable by human/nature etc Ex: have choice nodes that allow the system to select which action to do Choices made by utility and temporal consistency Can dynamically change decision on choices to satisfy consistency Qualitative State Plans (QSP) Introduce episodes: includes temporal constraint that tells how long something takes, includes state constraints. Ex: in between two events, we want to remain in a certain region Plan Completeness: All state constraints satisfied Summary TPN allows flexible choices QSP allows desirable states to write state-based goal-directed programs Generative planner: end and beginning states: set of actions and what it needs; the planner gives you a sequence of actions needed to reach goal Temporal networks allow us to dispatch events on the fly flexibly Temporal plan networks allow us to make choices on the fly Qualitative State Plans allow us to define states that we want to get to Uhura/d-Uhura helps diagnose over-subscribed plans to adapt Week 4 - June 22nd (No meeting due to DARPA Hackathon) # Week 3 - June 15th # Anna: Lessons learned from player recordings: (missed the first 2 points of Anna's pres. she can add here if possible) Experienced players make strategies and stick to them Non-experienced players: some make strategies, some do not; tend to pick up whatever they see in the game Most players tend to search left of the room; Some others randomly search left/right; Almost none search right first Time pressure makes strategy change, focusing on path to achieve goals Plan: Focus on maze for pilot 2 Isabelle: In summary, finished all tasks. Firstly, watched 2 sets of videos, answer 3 prompts related, developed summary of player behaviors. Secondly, finished assigned readings. Can classify player behavior into boxes: Some players will search and skim through the room Some others don't have a strategy Third group of people rush through the room, go through carefully later to see if they missed anyone Some people do a BFS way Some people do a DFS approach Players don't really stick to one approach Change over time, Ex: Maze isn't what they thought it would be, so they change their strategy Time constraint also changes their strategy Ex: they originally broke all gravel, now they break only enough gravel to search faster Optimizations for time For some players, this makes them change their strategy completely and makes them go random As players go on, some change to become more ordered rather than randomly doing things Players apply a type of strategy, then make decisions based on that Other things that affect gameplay: time constraints, how much information they have about map, pro/noob, personality of person Ex: player 1 shows she's a very thorough player by how she searches the maze very deeply How to predict player decisions: Identify player strategy Sometimes they start random so it's hard Other cases it's possible by watching how they play It's easier for people that stick to one particular strategy (usually more experienced players) Plan for this week: Working on running examples, changing parameters, see if can attempt to include some new player strategies discussed in 1v1 Focus on maze w more rooms Yang's comments on Isabelle: Some players have random actions, but can we still find patterns in this? Ex: some players break all gravel rather than just 2 blocks: why do they do this? Helps to understand the behavior better Find patterns in irrational behavior For rational behavior: there are always patterns that are not always rational Note for Christian: Identify reward function for different players, Reward for observing, reward for exploring new locations If cannot determine reward, what other explanations can you think of? Note for Darren: Write stories of each player Each mission is a story How is this ONE player is making decisions along the way How is this changing How is this player learning over the 5 tests: are they becoming more strategic, is their attention dropping Can use more symbols to describe this behavior Can talk about rules, heuristics, patterns, to describe this Darren: * Talked about ASU experiment, redacted because Christian did not yet do the experiment Christian: Pilot test 1: most interesting speaking in reward functions: Players that are more experienced are really interesting They make a lot of decisions with extreme investigation (opening a lot of things that in this particular task are not that effective, like opening hoppers, chests, etc) Heighten reward function on these misc functions In Christian's case, he skipped everything that others looked into, because he knew that he was looking for wool not things in chests and stuff Interesting that experienced players were walking around rather than sprinting even under time constraint Some experienced players were more prone to getting stressed: possible expectation on how they should do on the task Player 7 started breaking random stuff, dropped his inventory, complex chain of events that can be hard to predict Unexperienced players: Actions are random, once they learn how to open doors they only open doors to get to places More experienced players were more nuanced and interesting Yang's comments: Strategy is reward function: what are they driven by, what do they value and think is a waste of time Essie: Will present tomorrow at 1:30PM EST (A*) Week 2 - June 8th # (Taken by Darren and editted by Yang) Yang: Explanation of tom-minecraft tools and data folders: # Here are some details about the repository that's not documented so far but helpful to know. You should be able to see the codes and folders I mentioned after git pull . If not, let me know. Check main docs page https://zt-yang.github.io/tom-minecraft/ for updated talk schedules: This Tuesday and Friday OH, Tejas will give tutorials on numpy and Pytorch, and how value iterature algorithm is implemented Next Monday, Essie will give a talk on her MCTS & A* method for the gridworld search task tom-minecraft/documentation : Can make notes on how to solve common problems, or notes on reading to share with each other To publish changes: git push , PLEASE DON'T mkdocs deploy , otherwise you may override other people's changes! tom-minecraft/gridworld/recordings/_test cases output PNG when running visualize.py , yellow tiles represents the blocks that are exposed to the player (calculated by raycasting.py ) tom-minecraft/gridworld/trajectories output JSON when running visualize.py with PLANNING or HIERARCHICAL_PLANNING mode these JSON files can be input to visualize.py with INVERSE_PLANNING or EXPERIMENT_REPLAY mode tom-minecraft/gridworld/recordingsMalmo (not essential but useful): Human trajectory data are recorded by Malmo in observations.txt, include DistanceTravelled, etc. TXT trajectory data can be downloaded from dropbox into recordingsMalmo , will be processed by Recording replay.ipynb into JSON input for recording replay mode in visualize.py . For details see tools-map-generator.md . tom-minecraft/gridworld/recordingsMP4 (not essential but useful): Crop mp4.ipynb can segment recorded videos into smaller clips to give to human observers and ask: what will the player do next May also be useful for extracting images to train neural networks tom-minecraft/world-builder (not essential but useful): Tool for loading minecraft saves folder: load prebuild worlds Drag mca files into interface of https://pessimistress.github.io/minecraft/ (link is given in map-generator instructions ) to visualize and find the building you want to extract run-map-generator.ipynb generates map PNG, CSV, and JSON to represent the building (coordinates and block types) Can also generate MP4 videos of human trajectory on the map; can be real time to be used to make predictions of what they like Could be useful for UROPers to implement how algorithm is analyzing human data This week, will try to get UROPers to join ASU evaluation task: We don't know the test conditions, so it would be good to test with us (Darren and Christiam have signed up) Progress of UROPers: # Christian: Setup stuff, some problems with tom-minecraft and Malmo Python version 3.7.0 erroneous a bit Had issues with Yan'gs Minecraft Playground program crashing after running - Solution: make sure recordingsMalmo folder is created manually Darren: Installed tom-minecraft, Malmo, genesis Modified Malmo mod to track more data (door opening, block breaking) Played around with Genesis Talked with Yuening about how to make multi-agent recommendation systems Planned how to improve tracking to utilize multiplayer tracking Isabelle: Got through everything Finished prereq meetings Played search and rescue mission Still working on predicting player video predictions Question: When played search and rescue game: given seconds, then after it runs out still able to play? Countdown: data stops being collected at time 0, experiment officially ends, does not force player to quit Experimental replay: Will be shown later how to change to different algorithms: main purpose is to visualize human trajectory Can be replayed in Malmo to see the trajectory in real time, bird's eye view Help to visualize and compare human behavior Anna: Similar to Isabelle Setup everything Checked prereq reading list Understand MDP Went through videos, made observations while watching, did not make predictions; can probably do something more about it Problem: when running malmo.py: see myself in map, but feel like something's wrong with controls, cannot switch orientation Solution: press enter, then you can look around Note to Anna from Yang: if predictions made by Thursday, ready to hear Week 1 - June 2nd # (Taken by Yang) Self-introductions, see Minecraft UROP Team .","title":"Weekly meeting notes (to Week 8)"},{"location":"project-meetings/#weekly-meeting-minutes","text":"Here are meeting minutes of each weekly meeting. The most recent weeks come first.","title":"Weekly meeting minutes"},{"location":"project-meetings/#week-8-july-20th","text":"Darren: Start implementing his ideas Plan to incorporate Yuening\u2019s temporal framework for next week Problem he met: Notice that finding a route that get through all the rooms for the agent is a more complex problem then he thought Christian: Almost done with a function that could transfer our\u2019s mdp model to another mdp model Plan to work on generating reward function for agents in inverse planning framework Anna: * Implemented a memory system for agents, start to change how room values are assigned * Plan to continue exploring how to change room values so that agent can behave more \u2018human\u2019 Isabelle: * Tried a new algorithm, find out it didn\u2019t work * Plan to implement tile_level_value_iteration with a different algorithm","title":"Week 8 - July 20th"},{"location":"project-meetings/#week-7-july-13th","text":"Slides from UROP presentations Darren's slides on recommendation system: https://docs.google.com/presentation/d/1L5oIImOmCtqgyPgfelrYWuMUcwp_I3hb9LdaqXi8WI8/edit?usp=sharing Christian's slides on Bayesian inverse reinforcement learning: https://docs.google.com/presentation/d/1-BopTio00nzo1_8fXMAHf2oQlSPB4PY6iqJ4VEIOssQ/edit Anna's: https://docs.google.com/presentation/d/1BhOze5k_UC4YG-7O_cK7oKi7tf3q8eSeCSTwzGJu5UI/edit?usp=sharing Isabelle's: https://drive.google.com/drive/folders/1oMdqFolh-2R4-Ev9NmybcmvKsqRjwMnH?usp=sharing","title":"Week 7 - July 13th"},{"location":"project-meetings/#week-6-july-6th-no-meeting-due-to-independence-day-holidays","text":"","title":"Week 6 - July 6th (No meeting due to Independence Day holidays)"},{"location":"project-meetings/#week-5-june-29th","text":"Progress updates: Anna: Finished pilot test 3, gave observations and strategies Was looking into hierarchal code, found some problems and tried to fix Isabelle: Week 3 focused on how to model player behaviors observed Played around w parameters and reward functions Will post on slack channel Looked at hierarchal.py and ideas to implement her own branch Observed players (pilot test 3) as a participant, looked at results and wrote report, uploaded to git Christian: Did ASU test, conclusions are in meeting notes (maybe) Interesting that their victims died in the middle Strategy was suboptimal but didn\u2019t find out until failed at getting all of them Tejas provided lots of supplementary material Read Bayesian reinforcement paper Went over lecture notes from 6.437 Next step implementing bayesian learning Yang: Showed ASU data, will be useful Next Monday we will all give 15 min presentations on behaviors and planning 4-15 pages of slides to use videos/images to summarize plan Optional: give plan for computational model Yuening's presentation: Presentation on temporal networks In this case: temporal networks are using for mainly generative activity planning (or conditional planning, not being talked about) Mainly come up with sequence of actions to achieve plan Background on lab, MERS Goal: build robots that think for themselves and design how humans program cognitive robots ex: underwater rovers, mars rovers, autonomous vehicles Problem: failures occur Want to write high level programs that tell the robot what to achieve, then the robot can reason how to achieve this goal Ex: put out the fires at loc X and Y, then return in an hour. Avoid no fly zones. Here's a map. Programs are state based and model based Need to have planning and ability to execute plan Need to be aware of the risk and utility Simple temporal networks (STN) Scheduling representation We have events and temporal constraints (this occurs after that) This allows us to create flexible execution on the fly System can adapt: can have ranges of when events can occur based on temporal constraints How to detect inconsistencies: Two equalities: change constraint to two opposing directed edges (negate the one going away) Becomes a distance graph Simply check for negative cycles for inconsistencies (APSP) How to dispatch events Key idea: need to assign events in increasing order; once event is dispatched at time t, then you cannot dispatch anything before that anymore Negative edges from APSP dictate ordering constraints Ex: -1 negative edge means that the head needs to occur before the tail Then, propagate time constraint to other events and continue Simple temporal networks with uncertainty (STNU) Allows uncontrollable events Ex: cannot control when an event happens, but we know the delay between the previous event to the next event Three types of controllability: Weak, dynamic, and strong Dynamic: can generate schedule online given past uncontrollable durations Strong: generate schedule no matter how long the uncontrollables take Only if consistent or controllable can a schedule be found Example application: Uhura A user talks about their schedule then the planner looks for if consistent/controllable; if not, ask them to relax some requirements Algorithm is called temporal relaxation If inconsistent, extract negative cycles to find out how to resolve these conflicts Need to change neg cycle to not neg cycle by either: Discrete or Continuous relaxation Discrete: just don't satisfy that constraint Continuous: make it non negative by weakening the constraint (lower/upper bounds) Look for least costly relaxation Also: looking at d-Uhura which works in teams When ppl collaborate in tasks, they are constantly overloaded It's an oversubscription problem with multiple users where changes can have a rippling event d-Uhura helps teams adapt their schedule together Locally, try to get individuals to fix schedule without affecting others If not, collaborate with others Extensions: Temporal networks to temporal plan network Introduce choice nodes: controllable by agent, or uncontrollable by human/nature etc Ex: have choice nodes that allow the system to select which action to do Choices made by utility and temporal consistency Can dynamically change decision on choices to satisfy consistency Qualitative State Plans (QSP) Introduce episodes: includes temporal constraint that tells how long something takes, includes state constraints. Ex: in between two events, we want to remain in a certain region Plan Completeness: All state constraints satisfied Summary TPN allows flexible choices QSP allows desirable states to write state-based goal-directed programs Generative planner: end and beginning states: set of actions and what it needs; the planner gives you a sequence of actions needed to reach goal Temporal networks allow us to dispatch events on the fly flexibly Temporal plan networks allow us to make choices on the fly Qualitative State Plans allow us to define states that we want to get to Uhura/d-Uhura helps diagnose over-subscribed plans to adapt","title":"Week 5 - June 29th"},{"location":"project-meetings/#week-4-june-22nd-no-meeting-due-to-darpa-hackathon","text":"","title":"Week 4 - June 22nd (No meeting due to DARPA Hackathon)"},{"location":"project-meetings/#week-3-june-15th","text":"Anna: Lessons learned from player recordings: (missed the first 2 points of Anna's pres. she can add here if possible) Experienced players make strategies and stick to them Non-experienced players: some make strategies, some do not; tend to pick up whatever they see in the game Most players tend to search left of the room; Some others randomly search left/right; Almost none search right first Time pressure makes strategy change, focusing on path to achieve goals Plan: Focus on maze for pilot 2 Isabelle: In summary, finished all tasks. Firstly, watched 2 sets of videos, answer 3 prompts related, developed summary of player behaviors. Secondly, finished assigned readings. Can classify player behavior into boxes: Some players will search and skim through the room Some others don't have a strategy Third group of people rush through the room, go through carefully later to see if they missed anyone Some people do a BFS way Some people do a DFS approach Players don't really stick to one approach Change over time, Ex: Maze isn't what they thought it would be, so they change their strategy Time constraint also changes their strategy Ex: they originally broke all gravel, now they break only enough gravel to search faster Optimizations for time For some players, this makes them change their strategy completely and makes them go random As players go on, some change to become more ordered rather than randomly doing things Players apply a type of strategy, then make decisions based on that Other things that affect gameplay: time constraints, how much information they have about map, pro/noob, personality of person Ex: player 1 shows she's a very thorough player by how she searches the maze very deeply How to predict player decisions: Identify player strategy Sometimes they start random so it's hard Other cases it's possible by watching how they play It's easier for people that stick to one particular strategy (usually more experienced players) Plan for this week: Working on running examples, changing parameters, see if can attempt to include some new player strategies discussed in 1v1 Focus on maze w more rooms Yang's comments on Isabelle: Some players have random actions, but can we still find patterns in this? Ex: some players break all gravel rather than just 2 blocks: why do they do this? Helps to understand the behavior better Find patterns in irrational behavior For rational behavior: there are always patterns that are not always rational Note for Christian: Identify reward function for different players, Reward for observing, reward for exploring new locations If cannot determine reward, what other explanations can you think of? Note for Darren: Write stories of each player Each mission is a story How is this ONE player is making decisions along the way How is this changing How is this player learning over the 5 tests: are they becoming more strategic, is their attention dropping Can use more symbols to describe this behavior Can talk about rules, heuristics, patterns, to describe this Darren: * Talked about ASU experiment, redacted because Christian did not yet do the experiment Christian: Pilot test 1: most interesting speaking in reward functions: Players that are more experienced are really interesting They make a lot of decisions with extreme investigation (opening a lot of things that in this particular task are not that effective, like opening hoppers, chests, etc) Heighten reward function on these misc functions In Christian's case, he skipped everything that others looked into, because he knew that he was looking for wool not things in chests and stuff Interesting that experienced players were walking around rather than sprinting even under time constraint Some experienced players were more prone to getting stressed: possible expectation on how they should do on the task Player 7 started breaking random stuff, dropped his inventory, complex chain of events that can be hard to predict Unexperienced players: Actions are random, once they learn how to open doors they only open doors to get to places More experienced players were more nuanced and interesting Yang's comments: Strategy is reward function: what are they driven by, what do they value and think is a waste of time Essie: Will present tomorrow at 1:30PM EST (A*)","title":"Week 3 - June 15th"},{"location":"project-meetings/#week-2-june-8th","text":"(Taken by Darren and editted by Yang)","title":"Week 2 - June 8th"},{"location":"project-meetings/#yang-explanation-of-tom-minecraft-tools-and-data-folders","text":"Here are some details about the repository that's not documented so far but helpful to know. You should be able to see the codes and folders I mentioned after git pull . If not, let me know. Check main docs page https://zt-yang.github.io/tom-minecraft/ for updated talk schedules: This Tuesday and Friday OH, Tejas will give tutorials on numpy and Pytorch, and how value iterature algorithm is implemented Next Monday, Essie will give a talk on her MCTS & A* method for the gridworld search task tom-minecraft/documentation : Can make notes on how to solve common problems, or notes on reading to share with each other To publish changes: git push , PLEASE DON'T mkdocs deploy , otherwise you may override other people's changes! tom-minecraft/gridworld/recordings/_test cases output PNG when running visualize.py , yellow tiles represents the blocks that are exposed to the player (calculated by raycasting.py ) tom-minecraft/gridworld/trajectories output JSON when running visualize.py with PLANNING or HIERARCHICAL_PLANNING mode these JSON files can be input to visualize.py with INVERSE_PLANNING or EXPERIMENT_REPLAY mode tom-minecraft/gridworld/recordingsMalmo (not essential but useful): Human trajectory data are recorded by Malmo in observations.txt, include DistanceTravelled, etc. TXT trajectory data can be downloaded from dropbox into recordingsMalmo , will be processed by Recording replay.ipynb into JSON input for recording replay mode in visualize.py . For details see tools-map-generator.md . tom-minecraft/gridworld/recordingsMP4 (not essential but useful): Crop mp4.ipynb can segment recorded videos into smaller clips to give to human observers and ask: what will the player do next May also be useful for extracting images to train neural networks tom-minecraft/world-builder (not essential but useful): Tool for loading minecraft saves folder: load prebuild worlds Drag mca files into interface of https://pessimistress.github.io/minecraft/ (link is given in map-generator instructions ) to visualize and find the building you want to extract run-map-generator.ipynb generates map PNG, CSV, and JSON to represent the building (coordinates and block types) Can also generate MP4 videos of human trajectory on the map; can be real time to be used to make predictions of what they like Could be useful for UROPers to implement how algorithm is analyzing human data This week, will try to get UROPers to join ASU evaluation task: We don't know the test conditions, so it would be good to test with us (Darren and Christiam have signed up)","title":"Yang: Explanation of tom-minecraft tools and data folders:"},{"location":"project-meetings/#progress-of-uropers","text":"Christian: Setup stuff, some problems with tom-minecraft and Malmo Python version 3.7.0 erroneous a bit Had issues with Yan'gs Minecraft Playground program crashing after running - Solution: make sure recordingsMalmo folder is created manually Darren: Installed tom-minecraft, Malmo, genesis Modified Malmo mod to track more data (door opening, block breaking) Played around with Genesis Talked with Yuening about how to make multi-agent recommendation systems Planned how to improve tracking to utilize multiplayer tracking Isabelle: Got through everything Finished prereq meetings Played search and rescue mission Still working on predicting player video predictions Question: When played search and rescue game: given seconds, then after it runs out still able to play? Countdown: data stops being collected at time 0, experiment officially ends, does not force player to quit Experimental replay: Will be shown later how to change to different algorithms: main purpose is to visualize human trajectory Can be replayed in Malmo to see the trajectory in real time, bird's eye view Help to visualize and compare human behavior Anna: Similar to Isabelle Setup everything Checked prereq reading list Understand MDP Went through videos, made observations while watching, did not make predictions; can probably do something more about it Problem: when running malmo.py: see myself in map, but feel like something's wrong with controls, cannot switch orientation Solution: press enter, then you can look around Note to Anna from Yang: if predictions made by Thursday, ready to hear","title":"Progress of UROPers:"},{"location":"project-meetings/#week-1-june-2nd","text":"(Taken by Yang) Self-introductions, see Minecraft UROP Team .","title":"Week 1 - June 2nd"},{"location":"project-plans/","text":"Weekly plans for team and individuals # Here are lists of things you can do each week. The most recent weeks come first. You are not expected to finish them all and you can catch up with the remaining items the next week. We would like to hear your progress and blocks in following the lists at the our Mon 2 pm meeting of the week after the assigned week. Week 5 - Specify the behavior to model # For Everyone # In Week 6 team meeting (6 July), UROP students each has 15 minutes to present on player strategy/type they plan to model in 8 ASU test subjects . 4-15 pages of slides are recommended. Plan for computational model is optional. Presenter order: Anna, Isabelle, Christian, and Darren. Resources: - Minecraft World files of the two maps Week 4 - Meeting-free week! # For Everyone # Study the human prediction dataset from Pilot Test 3 and summarize your prediction rationale Watch the video clips in subfolders of player6 and player10 . At the end of each clip, try to predict what the player will do next (e.g., turn left/right/around, go straight, triage/give up the victim, clear the blockage...). Record your accuracy as if you are a test participant. Summarize your prediction strategy after watching the clips from each test of each player (e.g., according to what the player sees, or what he tends to do in the last mission). Compare your accuracy and prediction strategy with those of human observers Raw data of prediction questions, answers, and commentary: Google sheet . Let me know if you have questions about the data format in Slack channel #tom-minecraft-repo so everyone may benefit from the answer. Analysis of observer predictions: Google slides Summarize lessons learned; what are the different oberver type (just like player type, they prioritize different information for making prediction decisions) upload your summarizes through a markdown file named tom-minecraft/documentation/notes-YOUR_NAME.md by Week 4 Friday night. Yang will give everyone feedback and everyone will share a gist in Week 5 team meeting For Anna and Isabelle # Continue the task in Week 3, since there have been changes to the hierarchical planning algorithm. Based on the failure cases of the current hierarchical planning algorithm, find out why the agent get stuck, or moves to and back between two tiles, or choose a unintuitive path. Then make your git branch to change the framework (in visualize.py , hierarchical.py , mdp.py , mapreader.py ) For Darren # Continue the task in Week 3 For Christian # Study prerequisite readings for BIRL (details will be discussed in Week 3 individual meeting) Week 3 - Try out codes # For Anna and Isabelle # identify a set of player types by looking at the visualized player trajectories ( Download link from Dropbox ) look at the existing player profiles in player.py , try out different values for each of the attributes and run visualize.py with MODE = HIERARCHICAL_PLANNING, note down how each attribute affects the planning agent's behavior. create your own player profiles to replicate part of the human trajectories. Name your player profiles like 'systematic_A_2', which means the second systematic player profile used by Anne. We want to make the profiles generalizable both across players and within the different games of one player. You might need a sequence of player profiles to be able to assimilate one trajectory. To let the planning agent start from the time where the human changed player type, you can modify the map or create your own maps. summarize the limitations/failure cases of HIERARCHICAL_PLANNING and the current player profile attributes For Christian # go through this week's reading as sent via email by Tejas. go through the week 2 task for Pilot Test 2 after your ASU test, focusing on identifying how player's reward functions are different Use the visualized player trajectories ( Download link from Dropbox ) and game recordings For Darren # go through the week 2 task for Pilot Test 2 after your ASU test, focusing on craft stories of selected players and selected games. Use the visualized player trajectories ( Download link from Dropbox ) and game recordings it might be interesting to craft the story of one player across five missions if you find it hard to identify stories from Pilot Test 2 trajectories, you may use the narrated video recordings from Pilot Test 1 you may give names to the rooms and objects, e.g., it's a search specialist in a search and rescue mission, instead of chest-finding mission. try parsing your stories using Genesis Week 2 - Study human planning # For Anna, Christian, Isabelle # Finish watching the player videos and summarizing player behaviors/strategies. Generate theories about how to make decisions in the game and how to predict player decisions. Checkout the resources assigned to you at Human behavior in Minecraft SAR missions . Share your summaries and theories at your individual meeting this Thursday and team meeting next Monday. Use examples, screenshots, and video clips to illustrate your points. Be able to explain the inverse planning framework Check out weekly reading list . (Postponed until further notice) Create your own student directory inside tom-minecraft/UROPs based on the template directory provided inside. Create your own 12 by 12 map and three player configurations to test the planning algorithms and inverse planning framework. Add more modes or features to your local gridworld if capable. For Christain and Darren # Participate in a 2-hour ASU SAR Pilot experiment. Note down your decision making processes in the two 10-min missions. Think about how you might improve your performance (e.g., get more scores) if you were given a second chance to play each maze. Share your experience and strategic lessons learned at your individual meeting this Thursday (if possible) and team meeting next Monday. Use the maps of the buildings as your demo prop. Week 1 - Set up # For Anna, Christian, Isabelle, and Darren # Follow the instructions in 'SET IT UP' and 'RUN IT ONCE' You should be able to run the test examples in gridworld/visualize.py by following the test case instructions . Yang will demonstrate the test cases on Week 2 Monday 2:30-3pm You should be able to run gridworld/malmo.py and find yourself in a map that assimilates Stata center floor 1 Play the 'Singleplayer' Minecraft search and rescue games on your laptop You need to have set up Malmo and download the two world zipped files ( Dropbox link ) to your YOUR_WORKING_DIRECTORY/Malmo/Minecraft/run/saves/ folder, unzip, launch Malmo client, then find the world in the Minecraft interface -> Singleplayer mode. Give yourself 10 minutes to see how many victims you can find. Articulate your theories of human player's decision making process You are recommended to view the videos listed here . While you watch, try to predict what the player will do next. Try if your theories of how to make better predictions work by applying it in your mind when watching the next player. Be able to explain MDP and value iteration algorithm Check out prerequisite reading list . Tejas will answer questions about it on Week 2 Tuesday OH 1:30-2:30pm Don't hesitate to ask questions in Slack channel #tom-minecraft-repo and #reading-list, or directly message Yang. Slack is preferred over email :)","title":"Weekly plans (up to Week 4)"},{"location":"project-plans/#weekly-plans-for-team-and-individuals","text":"Here are lists of things you can do each week. The most recent weeks come first. You are not expected to finish them all and you can catch up with the remaining items the next week. We would like to hear your progress and blocks in following the lists at the our Mon 2 pm meeting of the week after the assigned week.","title":"Weekly plans for team and individuals"},{"location":"project-plans/#week-5-specify-the-behavior-to-model","text":"","title":"Week 5 - Specify the behavior to model"},{"location":"project-plans/#for-everyone","text":"In Week 6 team meeting (6 July), UROP students each has 15 minutes to present on player strategy/type they plan to model in 8 ASU test subjects . 4-15 pages of slides are recommended. Plan for computational model is optional. Presenter order: Anna, Isabelle, Christian, and Darren. Resources: - Minecraft World files of the two maps","title":"For Everyone"},{"location":"project-plans/#week-4-meeting-free-week","text":"","title":"Week 4 - Meeting-free week!"},{"location":"project-plans/#for-everyone_1","text":"Study the human prediction dataset from Pilot Test 3 and summarize your prediction rationale Watch the video clips in subfolders of player6 and player10 . At the end of each clip, try to predict what the player will do next (e.g., turn left/right/around, go straight, triage/give up the victim, clear the blockage...). Record your accuracy as if you are a test participant. Summarize your prediction strategy after watching the clips from each test of each player (e.g., according to what the player sees, or what he tends to do in the last mission). Compare your accuracy and prediction strategy with those of human observers Raw data of prediction questions, answers, and commentary: Google sheet . Let me know if you have questions about the data format in Slack channel #tom-minecraft-repo so everyone may benefit from the answer. Analysis of observer predictions: Google slides Summarize lessons learned; what are the different oberver type (just like player type, they prioritize different information for making prediction decisions) upload your summarizes through a markdown file named tom-minecraft/documentation/notes-YOUR_NAME.md by Week 4 Friday night. Yang will give everyone feedback and everyone will share a gist in Week 5 team meeting","title":"For Everyone"},{"location":"project-plans/#for-anna-and-isabelle","text":"Continue the task in Week 3, since there have been changes to the hierarchical planning algorithm. Based on the failure cases of the current hierarchical planning algorithm, find out why the agent get stuck, or moves to and back between two tiles, or choose a unintuitive path. Then make your git branch to change the framework (in visualize.py , hierarchical.py , mdp.py , mapreader.py )","title":"For Anna and Isabelle"},{"location":"project-plans/#for-darren","text":"Continue the task in Week 3","title":"For Darren"},{"location":"project-plans/#for-christian","text":"Study prerequisite readings for BIRL (details will be discussed in Week 3 individual meeting)","title":"For Christian"},{"location":"project-plans/#week-3-try-out-codes","text":"","title":"Week 3 - Try out codes"},{"location":"project-plans/#for-anna-and-isabelle_1","text":"identify a set of player types by looking at the visualized player trajectories ( Download link from Dropbox ) look at the existing player profiles in player.py , try out different values for each of the attributes and run visualize.py with MODE = HIERARCHICAL_PLANNING, note down how each attribute affects the planning agent's behavior. create your own player profiles to replicate part of the human trajectories. Name your player profiles like 'systematic_A_2', which means the second systematic player profile used by Anne. We want to make the profiles generalizable both across players and within the different games of one player. You might need a sequence of player profiles to be able to assimilate one trajectory. To let the planning agent start from the time where the human changed player type, you can modify the map or create your own maps. summarize the limitations/failure cases of HIERARCHICAL_PLANNING and the current player profile attributes","title":"For Anna and Isabelle"},{"location":"project-plans/#for-christian_1","text":"go through this week's reading as sent via email by Tejas. go through the week 2 task for Pilot Test 2 after your ASU test, focusing on identifying how player's reward functions are different Use the visualized player trajectories ( Download link from Dropbox ) and game recordings","title":"For Christian"},{"location":"project-plans/#for-darren_1","text":"go through the week 2 task for Pilot Test 2 after your ASU test, focusing on craft stories of selected players and selected games. Use the visualized player trajectories ( Download link from Dropbox ) and game recordings it might be interesting to craft the story of one player across five missions if you find it hard to identify stories from Pilot Test 2 trajectories, you may use the narrated video recordings from Pilot Test 1 you may give names to the rooms and objects, e.g., it's a search specialist in a search and rescue mission, instead of chest-finding mission. try parsing your stories using Genesis","title":"For Darren"},{"location":"project-plans/#week-2-study-human-planning","text":"","title":"Week 2 - Study human planning"},{"location":"project-plans/#for-anna-christian-isabelle","text":"Finish watching the player videos and summarizing player behaviors/strategies. Generate theories about how to make decisions in the game and how to predict player decisions. Checkout the resources assigned to you at Human behavior in Minecraft SAR missions . Share your summaries and theories at your individual meeting this Thursday and team meeting next Monday. Use examples, screenshots, and video clips to illustrate your points. Be able to explain the inverse planning framework Check out weekly reading list . (Postponed until further notice) Create your own student directory inside tom-minecraft/UROPs based on the template directory provided inside. Create your own 12 by 12 map and three player configurations to test the planning algorithms and inverse planning framework. Add more modes or features to your local gridworld if capable.","title":"For Anna, Christian, Isabelle"},{"location":"project-plans/#for-christain-and-darren","text":"Participate in a 2-hour ASU SAR Pilot experiment. Note down your decision making processes in the two 10-min missions. Think about how you might improve your performance (e.g., get more scores) if you were given a second chance to play each maze. Share your experience and strategic lessons learned at your individual meeting this Thursday (if possible) and team meeting next Monday. Use the maps of the buildings as your demo prop.","title":"For Christain and Darren"},{"location":"project-plans/#week-1-set-up","text":"","title":"Week 1 - Set up"},{"location":"project-plans/#for-anna-christian-isabelle-and-darren","text":"Follow the instructions in 'SET IT UP' and 'RUN IT ONCE' You should be able to run the test examples in gridworld/visualize.py by following the test case instructions . Yang will demonstrate the test cases on Week 2 Monday 2:30-3pm You should be able to run gridworld/malmo.py and find yourself in a map that assimilates Stata center floor 1 Play the 'Singleplayer' Minecraft search and rescue games on your laptop You need to have set up Malmo and download the two world zipped files ( Dropbox link ) to your YOUR_WORKING_DIRECTORY/Malmo/Minecraft/run/saves/ folder, unzip, launch Malmo client, then find the world in the Minecraft interface -> Singleplayer mode. Give yourself 10 minutes to see how many victims you can find. Articulate your theories of human player's decision making process You are recommended to view the videos listed here . While you watch, try to predict what the player will do next. Try if your theories of how to make better predictions work by applying it in your mind when watching the next player. Be able to explain MDP and value iteration algorithm Check out prerequisite reading list . Tejas will answer questions about it on Week 2 Tuesday OH 1:30-2:30pm Don't hesitate to ask questions in Slack channel #tom-minecraft-repo and #reading-list, or directly message Yang. Slack is preferred over email :)","title":"For Anna, Christian, Isabelle, and Darren"},{"location":"project-team/","text":"Minecraft UROP Team # Here is the digest for everyone's self-introductions. They include name, year, goal for the summer, and one thing fun that one did during quarantine. UROPs # Anna # A rising sophomore in 18-C. Located in the countryside of Cambridge. Interested in applying machine learning tools and search algorithms. Have tried fishing and growing plants. Christian # A rising junior in 6.3. Located in Seattle. Interested in applying machine learning tools. Learned flying a helicopter in the past months. Is getting pilot license in the following months. Darren # A rising sophomore in 6 and 9. Located in Seattle. Interested in language side of AI. Have made spicy burrito at home. Isabelle # A rising junior in 6. Located in Ghana, which has been hot until the raining season lately. This is her first Course 6 UROP. Used to sprint and have been trying to run 5k. Student Mentors # Essie # A 5th year (going up to my 6th year) graduate student in course 2 (Mechanical Engineering). Researched in power systems and had undergraduate education in math. Joined Josh Tenenbaum\u2019s CoCoSci (Computational Cognitive Science) group two years ago. Started working on this project with Yang around January/February. Interning this summer, so committing part time for the 3 months, but has flexible schedule to attend all technical meetings. Tejas # A first year PhD student in course 6. Part of the Signals, Information and Algorithms lab and advised by Greg Wornell. Research interests broadly fall under the category of Multimodal AI with a focus on applying modern deep learning and RL techniques to understand and test the limits of audiovisual systems. Started working with Yang on some aspects of this project through our research debugging sessions and the 6.246 class project. This summer he is also interning at Mitsubushi Electric Research Labs (MERL). Yang # A first year PhD student in course 6 and advised by Howie. Part of Genesis group for more than two years and worked on interactive task learning systems and strategic coaching systems. Interested in developing socially intelligent assistants and is working on this project for master thesis. Is taking improvisation comedy class virtually to fight Zoom awkwardness. Yuening # A 4th year PhD student in course 6 and in Brain's MERS (Model-Based Embedded and Robotics Systems) group. Research in multiagent negotiation in reaction to disturbances. PI # Howie # A research scientists at MIT for a long time. Located around Park Lane and have been biking.","title":"Team - Minecraft UROP"},{"location":"project-team/#minecraft-urop-team","text":"Here is the digest for everyone's self-introductions. They include name, year, goal for the summer, and one thing fun that one did during quarantine.","title":"Minecraft UROP Team"},{"location":"project-team/#urops","text":"","title":"UROPs"},{"location":"project-team/#anna","text":"A rising sophomore in 18-C. Located in the countryside of Cambridge. Interested in applying machine learning tools and search algorithms. Have tried fishing and growing plants.","title":"Anna"},{"location":"project-team/#christian","text":"A rising junior in 6.3. Located in Seattle. Interested in applying machine learning tools. Learned flying a helicopter in the past months. Is getting pilot license in the following months.","title":"Christian"},{"location":"project-team/#darren","text":"A rising sophomore in 6 and 9. Located in Seattle. Interested in language side of AI. Have made spicy burrito at home.","title":"Darren"},{"location":"project-team/#isabelle","text":"A rising junior in 6. Located in Ghana, which has been hot until the raining season lately. This is her first Course 6 UROP. Used to sprint and have been trying to run 5k.","title":"Isabelle"},{"location":"project-team/#student-mentors","text":"","title":"Student Mentors"},{"location":"project-team/#essie","text":"A 5th year (going up to my 6th year) graduate student in course 2 (Mechanical Engineering). Researched in power systems and had undergraduate education in math. Joined Josh Tenenbaum\u2019s CoCoSci (Computational Cognitive Science) group two years ago. Started working on this project with Yang around January/February. Interning this summer, so committing part time for the 3 months, but has flexible schedule to attend all technical meetings.","title":"Essie"},{"location":"project-team/#tejas","text":"A first year PhD student in course 6. Part of the Signals, Information and Algorithms lab and advised by Greg Wornell. Research interests broadly fall under the category of Multimodal AI with a focus on applying modern deep learning and RL techniques to understand and test the limits of audiovisual systems. Started working with Yang on some aspects of this project through our research debugging sessions and the 6.246 class project. This summer he is also interning at Mitsubushi Electric Research Labs (MERL).","title":"Tejas"},{"location":"project-team/#yang","text":"A first year PhD student in course 6 and advised by Howie. Part of Genesis group for more than two years and worked on interactive task learning systems and strategic coaching systems. Interested in developing socially intelligent assistants and is working on this project for master thesis. Is taking improvisation comedy class virtually to fight Zoom awkwardness.","title":"Yang"},{"location":"project-team/#yuening","text":"A 4th year PhD student in course 6 and in Brain's MERS (Model-Based Embedded and Robotics Systems) group. Research in multiagent negotiation in reaction to disturbances.","title":"Yuening"},{"location":"project-team/#pi","text":"","title":"PI"},{"location":"project-team/#howie","text":"A research scientists at MIT for a long time. Located around Park Lane and have been biking.","title":"Howie"},{"location":"project-vision/","text":"Vision - Team coacher agents # To develop socially intelligent team assistants, we need to enable them to infer humans\u2019 strategies, beliefs, and future actions. This page includes the behavior that we wish our agents can demonstrate. Agents that have Theory of Mind # Theory of mind is the ability to attribute mental states \u2014 beliefs, intentions, desires, emotions, knowledge, etc. \u2014 to oneself and to others. Tom opened a door. He found the way in was blocked by debris. He started clearing the debris. Agent: (Belief) Tom believed that there may be victims in the room. Agent: (Intention) Tom may wish to break through the debris. Agent: (Desire) Tom wants to follow his original search plan (because he didn't just turn away). Agent: (Emotion) Tom may feel stressful under time pressure and increasing workload. Agent: (Knowledge) Tom knows how to clear debris. Agents that have Bayesian theory of mind # The following graph may help us understand the inference and prediction process. Given the actions of one player, we want to infer his belief update model and POMDP planning model, then we can make predictions about future actions. Psychological theory Computational implementation Further readings: Baker, Chris, Rebecca Saxe, and Joshua Tenenbaum. \"Bayesian theory of mind: Modeling joint belief-desire attribution.\" , 2011. Baker, Chris, Rebecca Saxe, and Joshua Tenenbaum. \"Goal inference as inverse planning.\" , 2007. Agents that coach individuals based on their planning model # Potential ways for agents to improve singleplayer's taskwork by triggering them to think about their playing style: Agent: You seem to believe that you have searched every room on the west wing. Are you sure about that? Agent: You seem to have no prioritization of which rooms to search first. Do you have a preference? Agent: You seem to be fixated on your original plan. Do you have a strong reason to do that? Agents that coach teams with natural language # Potential ways for agents to improve teamwork using Theory of Mind: Request for help on behalf of a human team member based on the mental states of the other team members Agent: I think you are capable of helping him because you have the knowledge of clearing debris and I predict that you will have spare time after finishing your tasks. Explain a human team member's actions in terms of his mental states Agent: He can't come help you in the next 5 minutes because he is seems to have the intention of searching the west wing. Agents that learn lessons from stories # Stories include the histories of the human team members, the reports from past missions, and the experience from previous coaching. Stories help us think on higher levels and longer terms, find patterns, and encode knowledge. Stories can be generated by the agent based on the inference made and actions observed during a mission. They can be also collected from individual human team members after the mission. With stories, agents may make the following reasoning: Agent: Based on the previous 20 missions, humans tend to act less and less rationally after 3 minutes. Agent: Based on this member's previous 3 missions, he tends to act very greedily in the last minute. Agent: Based on my previous interactions with this team, I tend to improve the teamwork more if I give instructions instead of ask questions.","title":"Vision - Team coacher agents"},{"location":"project-vision/#vision-team-coacher-agents","text":"To develop socially intelligent team assistants, we need to enable them to infer humans\u2019 strategies, beliefs, and future actions. This page includes the behavior that we wish our agents can demonstrate.","title":"Vision - Team coacher agents"},{"location":"project-vision/#agents-that-have-theory-of-mind","text":"Theory of mind is the ability to attribute mental states \u2014 beliefs, intentions, desires, emotions, knowledge, etc. \u2014 to oneself and to others. Tom opened a door. He found the way in was blocked by debris. He started clearing the debris. Agent: (Belief) Tom believed that there may be victims in the room. Agent: (Intention) Tom may wish to break through the debris. Agent: (Desire) Tom wants to follow his original search plan (because he didn't just turn away). Agent: (Emotion) Tom may feel stressful under time pressure and increasing workload. Agent: (Knowledge) Tom knows how to clear debris.","title":"Agents that have Theory of Mind"},{"location":"project-vision/#agents-that-have-bayesian-theory-of-mind","text":"The following graph may help us understand the inference and prediction process. Given the actions of one player, we want to infer his belief update model and POMDP planning model, then we can make predictions about future actions. Psychological theory Computational implementation Further readings: Baker, Chris, Rebecca Saxe, and Joshua Tenenbaum. \"Bayesian theory of mind: Modeling joint belief-desire attribution.\" , 2011. Baker, Chris, Rebecca Saxe, and Joshua Tenenbaum. \"Goal inference as inverse planning.\" , 2007.","title":"Agents that have Bayesian theory of mind"},{"location":"project-vision/#agents-that-coach-individuals-based-on-their-planning-model","text":"Potential ways for agents to improve singleplayer's taskwork by triggering them to think about their playing style: Agent: You seem to believe that you have searched every room on the west wing. Are you sure about that? Agent: You seem to have no prioritization of which rooms to search first. Do you have a preference? Agent: You seem to be fixated on your original plan. Do you have a strong reason to do that?","title":"Agents that coach individuals based on their planning model"},{"location":"project-vision/#agents-that-coach-teams-with-natural-language","text":"Potential ways for agents to improve teamwork using Theory of Mind: Request for help on behalf of a human team member based on the mental states of the other team members Agent: I think you are capable of helping him because you have the knowledge of clearing debris and I predict that you will have spare time after finishing your tasks. Explain a human team member's actions in terms of his mental states Agent: He can't come help you in the next 5 minutes because he is seems to have the intention of searching the west wing.","title":"Agents that coach teams with natural language"},{"location":"project-vision/#agents-that-learn-lessons-from-stories","text":"Stories include the histories of the human team members, the reports from past missions, and the experience from previous coaching. Stories help us think on higher levels and longer terms, find patterns, and encode knowledge. Stories can be generated by the agent based on the inference made and actions observed during a mission. They can be also collected from individual human team members after the mission. With stories, agents may make the following reasoning: Agent: Based on the previous 20 missions, humans tend to act less and less rationally after 3 minutes. Agent: Based on this member's previous 3 missions, he tends to act very greedily in the last minute. Agent: Based on my previous interactions with this team, I tend to improve the teamwork more if I give instructions instead of ask questions.","title":"Agents that learn lessons from stories"},{"location":"setup-Malmo/","text":"Set up Malmo # Malmo is the platform we use for controlling agents in Minecraft and collecting data of Minecraft players. Follow the instructions to download the prebuilt version of Malmo, install the dependencies, and configure your JAVA_HOME environment variable. You can install Malmo at the same working directory as the tom-minecraft repository. Configure JAVA_HOME environment variable # To set JAVA_HOME on MacOS, follow the instructions . To set JAVA_HOME on Windows, press Windows key and search for \"Environmental Variables\" > New > Choose folder where you downloaded C:\\\\Program Files\\Java\\jdk1.8.0_251 When you check your Java version on terminal, it should show that you have JDK 1.8.0_*** depending on the Java SE you installed on Oracle site: (On MacOS) $ java -version openjdk version \"1.8.0_222\" OpenJDK Runtime Environment (AdoptOpenJDK)(build 1.8.0_222-b10) OpenJDK 64-Bit Server VM (AdoptOpenJDK)(build 25.222-b10, mixed mode) Test run Malmo environment # We are going to run an example maze that looks like our Stata center first floor. You need to have two terminal panels open. At one panel, launch Malmo client. (On MacOS) $ cd /YOUR_WORKING_DIRECTORY/Malmo/Minecraft $ ./launchClient.sh (On Windows) $ cd \\YOUR_WORKING_DIRECTORY\\Malmo\\Minecraft $ launchClient It might take you a minute when launching it for the first time. When you see the terminal showing > Building 95% > :runClient and a \"Minecraft 1.11.2\" window showing up, you are successful. Otherwise, check if your JAVA_HOME environment has been set properly At another panel, run our code for initiating a Minecraft world based on a csv file that specifies the map. (On MacOS) $ cd /YOUR_WORKING_DIRECTORY/tom-minecraft/gridworld $ python malmo.py You should see the following world being loaded in your \"Minecraft 1.11.2\" window: <<<<<<< HEAD ======= For MacOS # cd Malmo/Minecraft and run ./launchClient 7f4c4a3769bce8f618b6b613db82e6d8ce10ed60","title":"Set up Malmo"},{"location":"setup-Malmo/#set-up-malmo","text":"Malmo is the platform we use for controlling agents in Minecraft and collecting data of Minecraft players. Follow the instructions to download the prebuilt version of Malmo, install the dependencies, and configure your JAVA_HOME environment variable. You can install Malmo at the same working directory as the tom-minecraft repository.","title":"Set up Malmo"},{"location":"setup-Malmo/#configure-java_home-environment-variable","text":"To set JAVA_HOME on MacOS, follow the instructions . To set JAVA_HOME on Windows, press Windows key and search for \"Environmental Variables\" > New > Choose folder where you downloaded C:\\\\Program Files\\Java\\jdk1.8.0_251 When you check your Java version on terminal, it should show that you have JDK 1.8.0_*** depending on the Java SE you installed on Oracle site: (On MacOS) $ java -version openjdk version \"1.8.0_222\" OpenJDK Runtime Environment (AdoptOpenJDK)(build 1.8.0_222-b10) OpenJDK 64-Bit Server VM (AdoptOpenJDK)(build 25.222-b10, mixed mode)","title":"Configure JAVA_HOME environment variable"},{"location":"setup-Malmo/#test-run-malmo-environment","text":"We are going to run an example maze that looks like our Stata center first floor. You need to have two terminal panels open. At one panel, launch Malmo client. (On MacOS) $ cd /YOUR_WORKING_DIRECTORY/Malmo/Minecraft $ ./launchClient.sh (On Windows) $ cd \\YOUR_WORKING_DIRECTORY\\Malmo\\Minecraft $ launchClient It might take you a minute when launching it for the first time. When you see the terminal showing > Building 95% > :runClient and a \"Minecraft 1.11.2\" window showing up, you are successful. Otherwise, check if your JAVA_HOME environment has been set properly At another panel, run our code for initiating a Minecraft world based on a csv file that specifies the map. (On MacOS) $ cd /YOUR_WORKING_DIRECTORY/tom-minecraft/gridworld $ python malmo.py You should see the following world being loaded in your \"Minecraft 1.11.2\" window: <<<<<<< HEAD =======","title":"Test run Malmo environment"},{"location":"setup-Malmo/#for-macos","text":"cd Malmo/Minecraft and run ./launchClient 7f4c4a3769bce8f618b6b613db82e6d8ce10ed60","title":"For MacOS"},{"location":"setup-gridworld/","text":"Set up the gridworld # Folder tom-minecraft/gridworld/ includes most of the codes we use for inferring player planning model and predicting player actions through planning. It simplifies the Minecraft environment as a 2D gridworld, assuming that there will only be one object that the player interact with along the vertical axis. Install Python dependencies # First, you might need to install miniconda for setting up a Python virtual environment for this project. At terminal, create a python 3.7 virtual environment and activate it when testing codes (base) $ conda create --name mine python=3.7 (base) $ conda activate mine (mine) $ Use conda deactivate for exiting from the environment. Install required libraries # (mine) $ conda install matplotlib imageio jupyter pandas numpy scipy Pillow tqdm networkx moviepy future opencv (mine) $ conda install pytorch torchvision cpuonly -c pytorch (mine) $ pip install nbtlib mkdocs Refer to /gridworld/tutorials/ for some Jupyter Notebook examples of using numpy, matplotlib, and json libraries. Run a test case # Go to gridworld folder and run visualize.py , you should see the following Python Turtle window show up. If you are using macOS 10.6 or later, your mac may lead you to the computer's log in page once you run the command. Solutions can be found here . (For me, downgrading to python 3.7.0 fixes the problem). (mine) $ cd /YOUR_PATH_TO/tom-minecraft/gridworld (mine) $ python visualize.py Try out more test cases by following Planning test cases . Troubleshooting # tkinter on macOS 10.6 # If your python just crashes when you run visualize.py and your macOS version is 10.14.6 Mojave, you are experiencing a compatibility issue with your python version. The solutions can be found here. For Anna, downgrading to python from 3.7.7 to 3.7.0 fixed the problem.","title":"Set up gridworld"},{"location":"setup-gridworld/#set-up-the-gridworld","text":"Folder tom-minecraft/gridworld/ includes most of the codes we use for inferring player planning model and predicting player actions through planning. It simplifies the Minecraft environment as a 2D gridworld, assuming that there will only be one object that the player interact with along the vertical axis.","title":"Set up the gridworld"},{"location":"setup-gridworld/#install-python-dependencies","text":"First, you might need to install miniconda for setting up a Python virtual environment for this project. At terminal, create a python 3.7 virtual environment and activate it when testing codes (base) $ conda create --name mine python=3.7 (base) $ conda activate mine (mine) $ Use conda deactivate for exiting from the environment.","title":"Install Python dependencies"},{"location":"setup-gridworld/#install-required-libraries","text":"(mine) $ conda install matplotlib imageio jupyter pandas numpy scipy Pillow tqdm networkx moviepy future opencv (mine) $ conda install pytorch torchvision cpuonly -c pytorch (mine) $ pip install nbtlib mkdocs Refer to /gridworld/tutorials/ for some Jupyter Notebook examples of using numpy, matplotlib, and json libraries.","title":"Install required libraries"},{"location":"setup-gridworld/#run-a-test-case","text":"Go to gridworld folder and run visualize.py , you should see the following Python Turtle window show up. If you are using macOS 10.6 or later, your mac may lead you to the computer's log in page once you run the command. Solutions can be found here . (For me, downgrading to python 3.7.0 fixes the problem). (mine) $ cd /YOUR_PATH_TO/tom-minecraft/gridworld (mine) $ python visualize.py Try out more test cases by following Planning test cases .","title":"Run a test case"},{"location":"setup-gridworld/#troubleshooting","text":"","title":"Troubleshooting"},{"location":"setup-gridworld/#tkinter-on-macos-106","text":"If your python just crashes when you run visualize.py and your macOS version is 10.14.6 Mojave, you are experiencing a compatibility issue with your python version. The solutions can be found here. For Anna, downgrading to python from 3.7.7 to 3.7.0 fixed the problem.","title":"tkinter on macOS 10.6"},{"location":"setup-mkdocs/","text":"Set up the documentation # The documentation source files are in GenesisCore/documentation folder. Here is the layout of the documentation project: mkdocs.yml # The configuration file, e.g., for organizing the pages docs/ index.md # The documentation homepage ... # Other markdown pages imgs/ ... # Images css/ extra.css # style file to customize based on site/theme.css and site/theme_extra.css site/ # You can ignore thing here, MkDocs generNates them when you deploy the page Step 1: Install MkDocs # MkDocs is a Python package for generating static sites, especially good for the documentation style. Documentation source files are written in Markdown, and configured with a single YAML configuration file. You can install MkDocs using pip or other package manager. Install with pip # If you use the Python package manager, pip , then you can: $ pip install --upgrade pip $ pip install mkdocs To check that it is installed: $ mkdocs --version mkdocs, version 1.0.4 Install with a Package Manager # If you prefer useing another package manager (such as apt-get , dnf , homebrew , yum , chocolatey , etc.) to install packages on your system, then you may install MkDocs with your package manager. Step 2: Start MkDocs Server # MkDocs comes with a built-in dev-server that lets you preview your documentation as you work on it. Make sure you're in the same directory as the mkdocs.yml configuration file, and then start the server by running the mkdocs serve command: $ mkdocs serve INFO - Building documentation... INFO - Cleaning site directory [I 160402 15:50:43 server:271] Serving on http://127.0.0.1:8000 [I 160402 15:50:43 handlers:58] Start watching changes ... Now you can view the documentation site in http://127.0.0.1:8000/ , being updated in real time as you save your edits to the files. Step 3: Edit Pages # Before you make changes to the documentation, make sure to git pull the latest changes made by others to avoid conflict. Add a page # If you want to add a page to the wiki, reate a new markdown file in the docs/ directory. Then, add its name and markdown file name to mkdocs.yml , where you will see: nav: - Genesis: - Genesis System: index.md - Contribute to Documentation: mkdocs.md - Representation: - Inner Language - Innerese: innerese.md - ... - Fundamentals: - Inference by Rules: inference.md - ... - The current hierarchy and order is designed so that a new student can use the documentation as a tutorial. Major reorganization is possible upon discussion with Yang : Write in Markdown # Our wiki pages are written in Markdown , a lightweight markup language with plain text formatting syntax. The following cheatsheet designed by Theme Spectre showed the most frequently used syntax: To make it even easier to write in Markdown, you can add Markdown plugin to your editor. For example, I use markdown-editor in Atom to enjoy the following features: Toggle text styles using shortcut such as Ctrl+B Continue lists and table rows when press enter Correct ordered list numbers Step 4: Deploy After Editing # NOTE: At this moment, only Yang should deploy the changes to the website to avoid git merge problems. Please let her know of the changes you made. Summary # In this article, you have learned how to install the MkDocs package, write documentation pages, and deloy the site. Every time you make changes to the documentation, you need to use the following two commands at the project directory. $ git pull $ mkdocs serve # before editting, start the MkDocs server During editing, you add and organize pages at the configuration page, mkdocs.yml . After editing, you should let Yang know of the changes you made and git push","title":"Set up MkDocs"},{"location":"setup-mkdocs/#set-up-the-documentation","text":"The documentation source files are in GenesisCore/documentation folder. Here is the layout of the documentation project: mkdocs.yml # The configuration file, e.g., for organizing the pages docs/ index.md # The documentation homepage ... # Other markdown pages imgs/ ... # Images css/ extra.css # style file to customize based on site/theme.css and site/theme_extra.css site/ # You can ignore thing here, MkDocs generNates them when you deploy the page","title":"Set up the documentation"},{"location":"setup-mkdocs/#step-1-install-mkdocs","text":"MkDocs is a Python package for generating static sites, especially good for the documentation style. Documentation source files are written in Markdown, and configured with a single YAML configuration file. You can install MkDocs using pip or other package manager.","title":"Step 1: Install MkDocs"},{"location":"setup-mkdocs/#install-with-pip","text":"If you use the Python package manager, pip , then you can: $ pip install --upgrade pip $ pip install mkdocs To check that it is installed: $ mkdocs --version mkdocs, version 1.0.4","title":"Install with pip"},{"location":"setup-mkdocs/#install-with-a-package-manager","text":"If you prefer useing another package manager (such as apt-get , dnf , homebrew , yum , chocolatey , etc.) to install packages on your system, then you may install MkDocs with your package manager.","title":"Install with a Package Manager"},{"location":"setup-mkdocs/#step-2-start-mkdocs-server","text":"MkDocs comes with a built-in dev-server that lets you preview your documentation as you work on it. Make sure you're in the same directory as the mkdocs.yml configuration file, and then start the server by running the mkdocs serve command: $ mkdocs serve INFO - Building documentation... INFO - Cleaning site directory [I 160402 15:50:43 server:271] Serving on http://127.0.0.1:8000 [I 160402 15:50:43 handlers:58] Start watching changes ... Now you can view the documentation site in http://127.0.0.1:8000/ , being updated in real time as you save your edits to the files.","title":"Step 2: Start MkDocs Server"},{"location":"setup-mkdocs/#step-3-edit-pages","text":"Before you make changes to the documentation, make sure to git pull the latest changes made by others to avoid conflict.","title":"Step 3: Edit Pages"},{"location":"setup-mkdocs/#add-a-page","text":"If you want to add a page to the wiki, reate a new markdown file in the docs/ directory. Then, add its name and markdown file name to mkdocs.yml , where you will see: nav: - Genesis: - Genesis System: index.md - Contribute to Documentation: mkdocs.md - Representation: - Inner Language - Innerese: innerese.md - ... - Fundamentals: - Inference by Rules: inference.md - ... - The current hierarchy and order is designed so that a new student can use the documentation as a tutorial. Major reorganization is possible upon discussion with Yang :","title":"Add a page"},{"location":"setup-mkdocs/#write-in-markdown","text":"Our wiki pages are written in Markdown , a lightweight markup language with plain text formatting syntax. The following cheatsheet designed by Theme Spectre showed the most frequently used syntax: To make it even easier to write in Markdown, you can add Markdown plugin to your editor. For example, I use markdown-editor in Atom to enjoy the following features: Toggle text styles using shortcut such as Ctrl+B Continue lists and table rows when press enter Correct ordered list numbers","title":"Write in Markdown"},{"location":"setup-mkdocs/#step-4-deploy-after-editing","text":"NOTE: At this moment, only Yang should deploy the changes to the website to avoid git merge problems. Please let her know of the changes you made.","title":"Step 4: Deploy After Editing"},{"location":"setup-mkdocs/#summary","text":"In this article, you have learned how to install the MkDocs package, write documentation pages, and deloy the site. Every time you make changes to the documentation, you need to use the following two commands at the project directory. $ git pull $ mkdocs serve # before editting, start the MkDocs server During editing, you add and organize pages at the configuration page, mkdocs.yml . After editing, you should let Yang know of the changes you made and git push","title":"Summary"},{"location":"setup-repository/","text":"Set up the repository and documentation # To clone the GenesisCore repository, you need to send Yang your GitHub username to be invited as a collaborator. Open terminal and go to your favorite work directory (e.g., ~/Documents) to clone the project: git clone git@github.com:zt-yang/tom-minecraft.git (Optional) Install PyCharm IDE for debugging # I use PyCharm for more efficient debugging. It's free to use the Professional version after requesting student license. (Optional for MacOS) Install iTerm2 for splitting panels # I use iTerm2 as a replacement for Terminal because it supports multiple tabs and flexible panels when you right click. (Optional for MacOS) Install zsh for tab completion # I use zsh for configuring my terminal/iTerm2 app to make my command line experience more efficient. I love its command completion feature, which should be enabled by default. Important components of the repository # First-level folders # /gridworld ## main folder for our project /world-builder ## extract 2D map from Minecraft world folder /documentation ## run `mkdocs serve` to see the site offline ... gridworld folder # MalmoPython.so ## Malmo package file for MacOS MalmoPython.pyd ## Malmo package file for Windows (under construction)","title":"Set up the repository"},{"location":"setup-repository/#set-up-the-repository-and-documentation","text":"To clone the GenesisCore repository, you need to send Yang your GitHub username to be invited as a collaborator. Open terminal and go to your favorite work directory (e.g., ~/Documents) to clone the project: git clone git@github.com:zt-yang/tom-minecraft.git","title":"Set up the repository and documentation"},{"location":"setup-repository/#optional-install-pycharm-ide-for-debugging","text":"I use PyCharm for more efficient debugging. It's free to use the Professional version after requesting student license.","title":"(Optional) Install PyCharm IDE for debugging"},{"location":"setup-repository/#optional-for-macos-install-iterm2-for-splitting-panels","text":"I use iTerm2 as a replacement for Terminal because it supports multiple tabs and flexible panels when you right click.","title":"(Optional for MacOS) Install iTerm2 for splitting panels"},{"location":"setup-repository/#optional-for-macos-install-zsh-for-tab-completion","text":"I use zsh for configuring my terminal/iTerm2 app to make my command line experience more efficient. I love its command completion feature, which should be enabled by default.","title":"(Optional for MacOS) Install zsh for tab completion"},{"location":"setup-repository/#important-components-of-the-repository","text":"","title":"Important components of the repository"},{"location":"setup-repository/#first-level-folders","text":"/gridworld ## main folder for our project /world-builder ## extract 2D map from Minecraft world folder /documentation ## run `mkdocs serve` to see the site offline ...","title":"First-level folders"},{"location":"setup-repository/#gridworld-folder","text":"MalmoPython.so ## Malmo package file for MacOS MalmoPython.pyd ## Malmo package file for Windows (under construction)","title":"gridworld folder"},{"location":"tools-map-generator/","text":"Minecraft world - Map Generator # Note: The tools documented on this page has been migrated to git repository https://github.com/zt-yang/ASIST-MC-toolbox We have a tool tom-minecraft/world-builder/ for extracting the 2D floor plan of structures built in Minecraft and for visualizing human trajectory data on the map as a mp4 file. This folder includes the following components: run_map_generator.ipynb is the place to run the codes map_generator.py defines the functions used by run_map_generator.ipynb outputs/ stores the outputmaps and traces resources/ stores the icons and distionaries used to generate the maps Singleplayer/ is the the Minecraft world folder to be extracted from MCWorldLib.egg is a package we use for extracting region files ( Github ) Extract 2D maps # The input to map_generator.py include two parts: Minecraft world folder that's located in your Malmo folder MalmoPlatform/Minecraft/run/saves/ (using Malmo) or /Users/USER_NAME/Library/Application\\ Support/minecraft/saves/ (using licenced Minecraft Java Version). world = mc.load('Singleplayer') region = world.regions[-5,0] Inside the Singleplayer/region folder, there are .mca files. Each .mca file specifies 32 x 32 chunks. Each chunk specifies 16 x 16 x 16 blocks. In the above example, we located an SAR building in r.-5.0.mca , which we found using online tool Chunk viewer (as shown in the image below). On the interface, adjust the bar on the left to the highest, click the minimap on the bottom right side, and navigate to the light-colored areas representing artificial structures. After we found the .mca file, we need to further narrow down the blocks. the second part of the input is the ranges of x,y,z that specify the region of interests. This can be found by locating the building in the .mca file, then record down the coordinates of the top right and bottom left corner block on the ground (as the yellow/blue block shown in the image above). Then record down the lowest and highest level as y values. x_low = -2176 # x of the bottom left block x_high = -2097 # x of the top right block z_low = 144 # z of the top right block z_high = 207 # z of the bottom left block y_low = 52 y_high = 54 The outputs include three parts: images of the 2D floor plans on different levels of the region of interest at world-builder/outputs/*_map.png where * = 0,1,2,9 a json file specifying the block type of all blocks within the region of interest at world-builder/outputs/blocks_in_building.json # key = \"x,y,z\", value = block_type \"-2152,52,160\": \"air\", \"-2151,52,160\": \"stained_hardened_clay\", \"-2150,52,160\": \"anvil\", ... a csv file that represent the region of interest that can be used as input to our gridworld framework at world-builder/outputs/darpa_maze.csv Visualize human trajectories # Given DARPA testbed messages in the following format: raw-data 1.58507E+12 { app-id:\"TestbedBusInterface\", mission-id:\"6483fec4-153c-4994-8f42-a2e9b00d4db3\", routing-key:\"testbed-message\", testbed-message:{ msg:{ trial_id:\"6483fec4-153c-4994-8f42-a2e9b00d4db3\", sub_type:\"state\", source:\"simulator\", timestamp:\"2020-03-24T16:07:18.140779Z\" }, data:{ name:\"Player396\", entity_type:\"human\", life:20.0, x:-2193.5, y:23.0, z:194.6328125, pitch:0.0, yaw:0.0, motion_x:0.0, motion_y:0.0, motion_z:0.11038550616085588 } }, timestamp:1585066038141 } The codes visualize the location of the player on the map and infer the object that the player is interacting with. The codes further discretize the continuous position changes to generate a step by step trajectory that can be used as input to the inverse planning framework.","title":"Map Generator - Minecraft tools"},{"location":"tools-map-generator/#minecraft-world-map-generator","text":"Note: The tools documented on this page has been migrated to git repository https://github.com/zt-yang/ASIST-MC-toolbox We have a tool tom-minecraft/world-builder/ for extracting the 2D floor plan of structures built in Minecraft and for visualizing human trajectory data on the map as a mp4 file. This folder includes the following components: run_map_generator.ipynb is the place to run the codes map_generator.py defines the functions used by run_map_generator.ipynb outputs/ stores the outputmaps and traces resources/ stores the icons and distionaries used to generate the maps Singleplayer/ is the the Minecraft world folder to be extracted from MCWorldLib.egg is a package we use for extracting region files ( Github )","title":"Minecraft world - Map Generator"},{"location":"tools-map-generator/#extract-2d-maps","text":"The input to map_generator.py include two parts: Minecraft world folder that's located in your Malmo folder MalmoPlatform/Minecraft/run/saves/ (using Malmo) or /Users/USER_NAME/Library/Application\\ Support/minecraft/saves/ (using licenced Minecraft Java Version). world = mc.load('Singleplayer') region = world.regions[-5,0] Inside the Singleplayer/region folder, there are .mca files. Each .mca file specifies 32 x 32 chunks. Each chunk specifies 16 x 16 x 16 blocks. In the above example, we located an SAR building in r.-5.0.mca , which we found using online tool Chunk viewer (as shown in the image below). On the interface, adjust the bar on the left to the highest, click the minimap on the bottom right side, and navigate to the light-colored areas representing artificial structures. After we found the .mca file, we need to further narrow down the blocks. the second part of the input is the ranges of x,y,z that specify the region of interests. This can be found by locating the building in the .mca file, then record down the coordinates of the top right and bottom left corner block on the ground (as the yellow/blue block shown in the image above). Then record down the lowest and highest level as y values. x_low = -2176 # x of the bottom left block x_high = -2097 # x of the top right block z_low = 144 # z of the top right block z_high = 207 # z of the bottom left block y_low = 52 y_high = 54 The outputs include three parts: images of the 2D floor plans on different levels of the region of interest at world-builder/outputs/*_map.png where * = 0,1,2,9 a json file specifying the block type of all blocks within the region of interest at world-builder/outputs/blocks_in_building.json # key = \"x,y,z\", value = block_type \"-2152,52,160\": \"air\", \"-2151,52,160\": \"stained_hardened_clay\", \"-2150,52,160\": \"anvil\", ... a csv file that represent the region of interest that can be used as input to our gridworld framework at world-builder/outputs/darpa_maze.csv","title":"Extract 2D maps"},{"location":"tools-map-generator/#visualize-human-trajectories","text":"Given DARPA testbed messages in the following format: raw-data 1.58507E+12 { app-id:\"TestbedBusInterface\", mission-id:\"6483fec4-153c-4994-8f42-a2e9b00d4db3\", routing-key:\"testbed-message\", testbed-message:{ msg:{ trial_id:\"6483fec4-153c-4994-8f42-a2e9b00d4db3\", sub_type:\"state\", source:\"simulator\", timestamp:\"2020-03-24T16:07:18.140779Z\" }, data:{ name:\"Player396\", entity_type:\"human\", life:20.0, x:-2193.5, y:23.0, z:194.6328125, pitch:0.0, yaw:0.0, motion_x:0.0, motion_y:0.0, motion_z:0.11038550616085588 } }, timestamp:1585066038141 } The codes visualize the location of the player on the map and infer the object that the player is interacting with. The codes further discretize the continuous position changes to generate a step by step trajectory that can be used as input to the inverse planning framework.","title":"Visualize human trajectories"},{"location":"tut-Malmo/","text":"","title":"tut Malmo"},{"location":"tut-inverse/","text":"","title":"Tut inverse"},{"location":"tut-planning/","text":"All test cases # When before pushing changes in the gridworld/ repository or merging the git changes, we need to make sure that all test cases in gridworld/visualize.py can run successfully. To run the test cases, go to gridworld/visualize.py , and uncomment the MODEs between line 30 to line 41 to test different applications of the gridworld codes. MODE = 'PLANNING' ## normal VI # MODE = 'HIERARCHICAL_PLANNING' ## two-level VI # MODE = 'TILE_LEVEL_DFS' ## VI-DFS # MODE = 'ROOM_LEVEL_MCTS' ## VI-MCTS # MODE = 'INVERSE_PLANNING' ## default is two-level VI # MODE = 'EXPERIMENT_REPLAY' ## visualize human trajectory, generate PNG, or PNGs, or GIF # MODE = 'REPLAY_IN_MALMO' ## visualize in Malmo environment # MODE = 'REPLAY_DISCRETIZE' ## generate discretized trajectory file # MODE = 'EXPERIMENT_PARAM' ## test how parameters affect planning algorithms # MODE = 'EXPERIMENT_ALGO' ## test how different algorithms affect planning performance # MODE = 'PRINT_MAP' ## take screenshot of map, random initialize starting location # MODE = 'LEARNING' ## for reinforcement learning 1. Tabular # 1a. Value iteration (VI) # Run visualize.py as it is, you will see: Change PLAYER_NAME in line 62 from 'systematic' to 'with_dog' and 'fire_fighter' PLAYER_NAME = with_dog PLAYER_NAME = fire_fighter 2. Hierarchical # 2a. VI + VI # Comment out line 30 and uncomment line 31 # MODE = 'HIERARCHICAL_PLANNING' . Run visualize.py and you will see: 2b. VI + DFS # Comment out line 31 and uncomment line 32 # MODE = 'TILE_LEVEL_DFS' . Run visualize.py and you will see: 2c. VI + MCTS # Note: Please skip this one for now as I just created a bug that I need to fix. Comment out line 32 and uncomment line 33 # MODE = 'ROOM_LEVEL_MCTS' . Run visualize.py and you will see: 3. Inverse planning # Comment out line 33 and uncomment line 34 # MODE = 'INVERSE_PLANNING' . Run visualize.py with PLAYER_NAME in line 62 equals to 'fire_fighter' and 'with_dog': PLAYER_NAME = fire_fighter PLAYER_NAME = with_dog 4. Experiment replay # This mode visualizes all of the human trajectories (e.g., _player10 shaoying_test2.json ) in gridworld/trajectories/24by24/ :","title":"Planning-related test cases"},{"location":"tut-planning/#all-test-cases","text":"When before pushing changes in the gridworld/ repository or merging the git changes, we need to make sure that all test cases in gridworld/visualize.py can run successfully. To run the test cases, go to gridworld/visualize.py , and uncomment the MODEs between line 30 to line 41 to test different applications of the gridworld codes. MODE = 'PLANNING' ## normal VI # MODE = 'HIERARCHICAL_PLANNING' ## two-level VI # MODE = 'TILE_LEVEL_DFS' ## VI-DFS # MODE = 'ROOM_LEVEL_MCTS' ## VI-MCTS # MODE = 'INVERSE_PLANNING' ## default is two-level VI # MODE = 'EXPERIMENT_REPLAY' ## visualize human trajectory, generate PNG, or PNGs, or GIF # MODE = 'REPLAY_IN_MALMO' ## visualize in Malmo environment # MODE = 'REPLAY_DISCRETIZE' ## generate discretized trajectory file # MODE = 'EXPERIMENT_PARAM' ## test how parameters affect planning algorithms # MODE = 'EXPERIMENT_ALGO' ## test how different algorithms affect planning performance # MODE = 'PRINT_MAP' ## take screenshot of map, random initialize starting location # MODE = 'LEARNING' ## for reinforcement learning","title":"All test cases"},{"location":"tut-planning/#1-tabular","text":"","title":"1. Tabular"},{"location":"tut-planning/#1a-value-iteration-vi","text":"Run visualize.py as it is, you will see: Change PLAYER_NAME in line 62 from 'systematic' to 'with_dog' and 'fire_fighter' PLAYER_NAME = with_dog PLAYER_NAME = fire_fighter","title":"1a. Value iteration (VI)"},{"location":"tut-planning/#2-hierarchical","text":"","title":"2. Hierarchical"},{"location":"tut-planning/#2a-vi-vi","text":"Comment out line 30 and uncomment line 31 # MODE = 'HIERARCHICAL_PLANNING' . Run visualize.py and you will see:","title":"2a. VI + VI"},{"location":"tut-planning/#2b-vi-dfs","text":"Comment out line 31 and uncomment line 32 # MODE = 'TILE_LEVEL_DFS' . Run visualize.py and you will see:","title":"2b. VI + DFS"},{"location":"tut-planning/#2c-vi-mcts","text":"Note: Please skip this one for now as I just created a bug that I need to fix. Comment out line 32 and uncomment line 33 # MODE = 'ROOM_LEVEL_MCTS' . Run visualize.py and you will see:","title":"2c. VI + MCTS"},{"location":"tut-planning/#3-inverse-planning","text":"Comment out line 33 and uncomment line 34 # MODE = 'INVERSE_PLANNING' . Run visualize.py with PLAYER_NAME in line 62 equals to 'fire_fighter' and 'with_dog': PLAYER_NAME = fire_fighter PLAYER_NAME = with_dog","title":"3. Inverse planning"},{"location":"tut-planning/#4-experiment-replay","text":"This mode visualizes all of the human trajectories (e.g., _player10 shaoying_test2.json ) in gridworld/trajectories/24by24/ :","title":"4. Experiment replay"},{"location":"tut-rl/","text":"","title":"Tut rl"}]}